{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    0.990099\n",
      "1    0.990099\n",
      "2    0.990099\n",
      "3    1.000000\n",
      "4    0.990099\n",
      "5    1.000000\n",
      "6    1.188119\n",
      "7    1.782178\n",
      "Name: x01, dtype: float64 0    0.896552\n",
      "1    1.000000\n",
      "2    1.055172\n",
      "3    0.896552\n",
      "4    1.000000\n",
      "5    1.055172\n",
      "6    0.896552\n",
      "7    1.000000\n",
      "Name: x02, dtype: float64 0    1.009091\n",
      "1    1.000000\n",
      "2    0.993506\n",
      "3    1.009091\n",
      "4    1.000000\n",
      "5    0.993506\n",
      "6    1.009091\n",
      "7    1.000000\n",
      "Name: x03, dtype: float64 0    0.955835\n",
      "1    0.996883\n",
      "2    0.972192\n",
      "3    0.953983\n",
      "4    1.003055\n",
      "5    0.969106\n",
      "6    1.098423\n",
      "7    1.432055\n",
      "Name: y3, dtype: float64\n",
      "[[0.9900990099009901, 0.896551724137931, 1.009090909090909], [0.9900990099009901, 1.0, 1.0], [0.9900990099009901, 1.0551724137931036, 0.9935064935064936], [1.0, 0.896551724137931, 1.009090909090909], [0.9900990099009901, 1.0, 1.0], [1.0, 1.0551724137931036, 0.9935064935064936], [1.188118811881188, 0.896551724137931, 1.009090909090909], [1.7821782178217822, 1.0, 1.0]]\n",
      "[[0.99009901 0.89655172 1.00909091]\n",
      " [0.99009901 1.         1.        ]\n",
      " [0.99009901 1.05517241 0.99350649]\n",
      " [1.         0.89655172 1.00909091]\n",
      " [0.99009901 1.         1.        ]\n",
      " [1.         1.05517241 0.99350649]\n",
      " [1.18811881 0.89655172 1.00909091]\n",
      " [1.78217822 1.         1.        ]]\n"
     ]
    }
   ],
   "source": [
    "'''>>>>> start CodeP2.2\n",
    "    V.P. Carey ME249, Spring 2021\n",
    "\n",
    "Intro to Neural Network Modeling \n",
    "Keras model for comparison with first principles model'''\n",
    "\n",
    "#import useful packages\n",
    "import keras\n",
    "import pandas as pd\n",
    "from keras.models import Sequential\n",
    "import numpy as np\n",
    "import keras.backend as kb\n",
    "import tensorflow as tf\n",
    "\n",
    "#the follwoing 2 lines are only needed for Mac OS machines\n",
    "# import os\n",
    "# os.environ['KMP_DUPLICATE_LIB_OK']='True'\n",
    "\n",
    "#raw data in dictionary form x01, x02, x03, y3\n",
    "my_dict = { \n",
    "    'x01' : [20., 20., 20., 20.2, 20., 20.2, 24.0, 36.],\n",
    "    'x02' : [13., 14.5, 15.3, 13., 14.5, 15.3, 13., 14.5],\n",
    "    'x03' : [310.8, 308.0, 306.0, 310.8, 308.0, 306.0, 310.8, 308.0],\n",
    "    'y3' : [30.97, 32.3, 31.5, 30.91, 32.5, 31.4, 35.59, 46.4]\n",
    "}\n",
    "#normalized inputs in array\n",
    "xdata = []\n",
    "xdata = [[20./20.2, 13.0/14.5, 310.8/308.0], [20./20.2, 14.5/14.5, 308.0/308.0]] \n",
    "xdata.append([20./20.2, 15.3/14.5, 306.0/308.0])\n",
    "xdata.append([20.2/20.2, 13.0/14.5, 310.8/308.0]) \n",
    "xdata.append([20./20.2, 14.5/14.5, 308.0/308.0]) \n",
    "xdata.append([20.2/20.2, 15.3/14.5, 306.0/308.0]) \n",
    "xdata.append([24./20.2, 13.0/14.5, 310.8/308.0]) \n",
    "xdata.append([36./20.2, 14.5/14.5, 308.0/308.0]) \n",
    "#data frame\n",
    "df = pd.DataFrame(my_dict)\n",
    "#devide by the median to normalize \n",
    "df.x01= df.x01/20.2\n",
    "df.x02= df.x02/14.5\n",
    "df.x03= df.x03/308.0\n",
    "#normalize output array\n",
    "df.y3= df.y3/32.401\n",
    "df.head\n",
    "print (df.x01, df.x02, df.x03, df.y3)\n",
    "\n",
    "xarray= np.array(xdata)\n",
    "print (xdata)\n",
    "print (xarray)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define model\n",
    "\n",
    "#As seen below, we have created three dense layers each with just one neuron. \n",
    "#A dense layer is a layer in neural network that’s fully connected. \n",
    "#In other words, all the neurons in one layer are connected to all other neurons in the next layer.\n",
    "#In the first layer, we need to provide the input shape, which is 3 in this case. \n",
    "#The activation function we have chosen is ReLU, which stands for rectified linear unit.\n",
    "\n",
    "from keras import backend as K\n",
    "#initialize weights with values between -0.2 and 1.2\n",
    "initializer = tf.keras.initializers.RandomUniform(minval= -0.2, maxval=1.2)\n",
    "\n",
    "# define three layer model with one neuron in each layer\n",
    "model = keras.Sequential([\n",
    "    keras.layers.Dense(1, activation=K.elu, input_shape=[3]),\n",
    "    keras.layers.Dense(1, activation=K.elu),\n",
    "    keras.layers.Dense(1, activation=K.elu)\n",
    "  ])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We’re using RMSprop as our optimizer here. RMSprop stands for Root Mean Square Propagation. \n",
    "#It’s one of the most popular gradient descent optimization algorithms for deep learning networks. \n",
    "#RMSprop is an optimizer that’s reliable and fast.\n",
    "#We’re compiling the mode using the model.compile function. The loss function used here \n",
    "#is mean absolute error. After the compilation of the model, we’ll use the fit method with 100 epochs.\n",
    "\n",
    "#Running model.fit successive times extends the calculation to addtional epochs.\n",
    "\n",
    "sgd = keras.optimizers.RMSprop(learning_rate=0.1)\n",
    "model.compile(loss='mean_absolute_error',optimizer=sgd)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#After the compilation of the model, we’ll use the fit method with 500 epochs.\n",
    "#I started with epochs value of 100 and then tested the model after training. \n",
    "#The prediction was not that good. Then I modified the number of epochs to 200 and tested the model again. \n",
    "#Accuracy had improved slightly, but figured I’d give it one more try. Finally, at 500 epochs \n",
    "#I found acceptable prediction accuracy.\n",
    "\n",
    "#The fit method takes three parameters; namely, x, y, and number of epochs. \n",
    "#During model training, if all the batches of data are seen by the model once, \n",
    "#we say that one epoch has been completed.\n",
    "\n",
    "# Add an early stopping callback\n",
    "es = keras.callbacks.EarlyStopping(\n",
    "    monitor='loss', \n",
    "    mode='min', \n",
    "    patience = 20, \n",
    "    restore_best_weights = True, \n",
    "    verbose=1)\n",
    "# Add a checkpoint where loss is minimum, and save that model\n",
    "mc = keras.callbacks.ModelCheckpoint('best_model.SB', monitor='loss', \n",
    "                     mode='min',  verbose=1, save_best_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "8/8 [==============================] - 1s 118ms/step - loss: 1.1973\n",
      "Epoch 2/100\n",
      "8/8 [==============================] - 0s 566us/step - loss: 0.7643\n",
      "Epoch 3/100\n",
      "8/8 [==============================] - 0s 475us/step - loss: 0.4853\n",
      "Epoch 4/100\n",
      "8/8 [==============================] - 0s 587us/step - loss: 0.2830\n",
      "Epoch 5/100\n",
      "8/8 [==============================] - 0s 692us/step - loss: 0.1006\n",
      "Epoch 6/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.1650\n",
      "Epoch 7/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.0853\n",
      "Epoch 8/100\n",
      "8/8 [==============================] - 0s 744us/step - loss: 0.0849\n",
      "Epoch 9/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.0846\n",
      "Epoch 10/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.0842\n",
      "Epoch 11/100\n",
      "8/8 [==============================] - 0s 638us/step - loss: 0.0838\n",
      "Epoch 12/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0833\n",
      "Epoch 13/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.0830\n",
      "Epoch 14/100\n",
      "8/8 [==============================] - 0s 683us/step - loss: 0.0988\n",
      "Epoch 15/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1403\n",
      "Epoch 16/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.1461\n",
      "Epoch 17/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.0968\n",
      "Epoch 18/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1195\n",
      "Epoch 19/100\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0836\n",
      "Epoch 20/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.0829\n",
      "Epoch 21/100\n",
      "8/8 [==============================] - 0s 804us/step - loss: 0.0819\n",
      "Epoch 22/100\n",
      "8/8 [==============================] - 0s 520us/step - loss: 0.0803\n",
      "Epoch 23/100\n",
      "8/8 [==============================] - 0s 464us/step - loss: 0.0779\n",
      "Epoch 24/100\n",
      "8/8 [==============================] - 0s 497us/step - loss: 0.0737\n",
      "Epoch 25/100\n",
      "8/8 [==============================] - 0s 684us/step - loss: 0.0693\n",
      "Epoch 26/100\n",
      "8/8 [==============================] - 0s 621us/step - loss: 0.3070\n",
      "Epoch 27/100\n",
      "8/8 [==============================] - 0s 647us/step - loss: 0.1119\n",
      "Epoch 28/100\n",
      "8/8 [==============================] - 0s 562us/step - loss: 0.0982\n",
      "Epoch 29/100\n",
      "8/8 [==============================] - 0s 510us/step - loss: 0.1594\n",
      "Epoch 30/100\n",
      "8/8 [==============================] - 0s 739us/step - loss: 0.0876\n",
      "Epoch 31/100\n",
      "8/8 [==============================] - 0s 695us/step - loss: 0.1265\n",
      "Epoch 32/100\n",
      "8/8 [==============================] - 0s 466us/step - loss: 0.1240\n",
      "Epoch 33/100\n",
      "8/8 [==============================] - 0s 910us/step - loss: 0.0817\n",
      "Epoch 34/100\n",
      "8/8 [==============================] - 0s 930us/step - loss: 0.0792\n",
      "Epoch 35/100\n",
      "8/8 [==============================] - 0s 797us/step - loss: 0.0751\n",
      "Epoch 36/100\n",
      "8/8 [==============================] - 0s 613us/step - loss: 0.0713\n",
      "Epoch 37/100\n",
      "8/8 [==============================] - 0s 495us/step - loss: 0.2465\n",
      "Epoch 38/100\n",
      "8/8 [==============================] - 0s 692us/step - loss: 0.1190\n",
      "Epoch 39/100\n",
      "8/8 [==============================] - 0s 349us/step - loss: 0.0818\n",
      "Epoch 40/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.0792\n",
      "Epoch 41/100\n",
      "8/8 [==============================] - 0s 966us/step - loss: 0.0750\n",
      "Epoch 42/100\n",
      "8/8 [==============================] - 0s 419us/step - loss: 0.0680\n",
      "Epoch 43/100\n",
      "8/8 [==============================] - 0s 678us/step - loss: 0.0604\n",
      "Epoch 44/100\n",
      "8/8 [==============================] - 0s 707us/step - loss: 0.2992\n",
      "Epoch 45/100\n",
      "8/8 [==============================] - 0s 576us/step - loss: 0.1086\n",
      "Epoch 46/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.0840\n",
      "Epoch 47/100\n",
      "8/8 [==============================] - 0s 476us/step - loss: 0.1493\n",
      "Epoch 48/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.0828\n",
      "Epoch 49/100\n",
      "8/8 [==============================] - 0s 757us/step - loss: 0.1083\n",
      "Epoch 50/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.0830\n",
      "Epoch 51/100\n",
      "8/8 [==============================] - 0s 573us/step - loss: 0.1506\n",
      "Epoch 52/100\n",
      "8/8 [==============================] - 0s 570us/step - loss: 0.0827\n",
      "Epoch 53/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.0808\n",
      "Epoch 54/100\n",
      "8/8 [==============================] - 0s 882us/step - loss: 0.1077\n",
      "Epoch 55/100\n",
      "8/8 [==============================] - 0s 858us/step - loss: 0.0870\n",
      "Epoch 56/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.1921\n",
      "Epoch 57/100\n",
      "8/8 [==============================] - 0s 478us/step - loss: 0.1008\n",
      "Epoch 58/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.1233\n",
      "Epoch 59/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.1237\n",
      "Epoch 60/100\n",
      "8/8 [==============================] - 0s 671us/step - loss: 0.0923\n",
      "Epoch 61/100\n",
      "8/8 [==============================] - 0s 823us/step - loss: 0.1477\n",
      "Epoch 62/100\n",
      "8/8 [==============================] - 0s 819us/step - loss: 0.0858\n",
      "Epoch 63/100\n",
      "8/8 [==============================] - 0s 510us/step - loss: 0.1118\n",
      "Restoring model weights from the end of the best epoch\n",
      "Epoch 00063: early stopping\n",
      "best epoch =  43\n",
      "smallest loss = 0.06043669581413269\n"
     ]
    }
   ],
   "source": [
    "historyData = model.fit(xarray,df.y3,epochs=100,callbacks=[es])\n",
    "\n",
    "loss_hist = historyData.history['loss']\n",
    "#The above line will return a dictionary, access it's info like this:\n",
    "best_epoch = np.argmin(historyData.history['loss']) + 1\n",
    "print ('best epoch = ', best_epoch)\n",
    "print('smallest loss =', np.min(loss_hist))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "8/8 [==============================] - 0s 752us/step - loss: 0.2992\n",
      "Epoch 2/100\n",
      "8/8 [==============================] - 0s 746us/step - loss: 0.0805\n",
      "Epoch 3/100\n",
      "8/8 [==============================] - 0s 519us/step - loss: 0.1172\n",
      "Epoch 4/100\n",
      "8/8 [==============================] - 0s 571us/step - loss: 0.1283\n",
      "Epoch 5/100\n",
      "8/8 [==============================] - 0s 847us/step - loss: 0.0836\n",
      "Epoch 6/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.0794\n",
      "Epoch 7/100\n",
      "8/8 [==============================] - 0s 818us/step - loss: 0.0741\n",
      "Epoch 8/100\n",
      "8/8 [==============================] - 0s 891us/step - loss: 0.0838\n",
      "Epoch 9/100\n",
      "8/8 [==============================] - 0s 996us/step - loss: 0.1414\n",
      "Epoch 10/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.1446\n",
      "Epoch 11/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0891\n",
      "Epoch 12/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.0834\n",
      "Epoch 13/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.0780\n",
      "Epoch 14/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.0704\n",
      "Epoch 15/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0573\n",
      "Epoch 16/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0438\n",
      "Epoch 17/100\n",
      "8/8 [==============================] - 0s 951us/step - loss: 0.3506\n",
      "Epoch 18/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0806\n",
      "Epoch 19/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.0772\n",
      "Epoch 20/100\n",
      "8/8 [==============================] - 0s 650us/step - loss: 0.0850\n",
      "Epoch 21/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.1409\n",
      "Epoch 22/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.1507\n",
      "Epoch 23/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.0738\n",
      "Epoch 24/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0634\n",
      "Epoch 25/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.0469\n",
      "Epoch 26/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.1660\n",
      "Epoch 27/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.2044\n",
      "Epoch 28/100\n",
      "8/8 [==============================] - 0s 655us/step - loss: 0.1209\n",
      "Epoch 29/100\n",
      "8/8 [==============================] - 0s 554us/step - loss: 0.1066\n",
      "Epoch 30/100\n",
      "8/8 [==============================] - 0s 557us/step - loss: 0.1639\n",
      "Epoch 31/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.0824\n",
      "Epoch 32/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.0808\n",
      "Epoch 33/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.0872\n",
      "Epoch 34/100\n",
      "8/8 [==============================] - 0s 713us/step - loss: 0.1364\n",
      "Epoch 35/100\n",
      "8/8 [==============================] - 0s 648us/step - loss: 0.1270\n",
      "Epoch 36/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0781\n",
      "Restoring model weights from the end of the best epoch\n",
      "Epoch 00036: early stopping\n",
      "best epoch =  16\n",
      "smallest loss = 0.04379470646381378\n"
     ]
    }
   ],
   "source": [
    "historyData = model.fit(xarray,df.y3,epochs=100,callbacks=[es])\n",
    "\n",
    "loss_hist = historyData.history['loss']\n",
    "#The above line will return a dictionary, access it's info like this:\n",
    "best_epoch = np.argmin(historyData.history['loss']) + 1\n",
    "print ('best epoch = ', best_epoch)\n",
    "print('smallest loss =', np.min(loss_hist))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "8/8 [==============================] - 1s 107ms/step - loss: 0.3506\n",
      "Epoch 2/200\n",
      "8/8 [==============================] - 0s 569us/step - loss: 0.2883\n",
      "Epoch 3/200\n",
      "8/8 [==============================] - 0s 481us/step - loss: 0.2450\n",
      "Epoch 4/200\n",
      "8/8 [==============================] - 0s 838us/step - loss: 0.2103\n",
      "Epoch 5/200\n",
      "8/8 [==============================] - 0s 816us/step - loss: 0.1805\n",
      "Epoch 6/200\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1540\n",
      "Epoch 7/200\n",
      "8/8 [==============================] - 0s 827us/step - loss: 0.1299\n",
      "Epoch 8/200\n",
      "8/8 [==============================] - 0s 920us/step - loss: 0.1076\n",
      "Epoch 9/200\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.0866\n",
      "Epoch 10/200\n",
      "8/8 [==============================] - 0s 846us/step - loss: 0.0717\n",
      "Epoch 11/200\n",
      "8/8 [==============================] - 0s 845us/step - loss: 0.0664\n",
      "Epoch 12/200\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0647\n",
      "Epoch 13/200\n",
      "8/8 [==============================] - 0s 672us/step - loss: 0.0635\n",
      "Epoch 14/200\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.0623\n",
      "Epoch 15/200\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0610\n",
      "Epoch 16/200\n",
      "8/8 [==============================] - 0s 831us/step - loss: 0.0595\n",
      "Epoch 17/200\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.0579\n",
      "Epoch 18/200\n",
      "8/8 [==============================] - 0s 627us/step - loss: 0.0562\n",
      "Epoch 19/200\n",
      "8/8 [==============================] - 0s 890us/step - loss: 0.0544\n",
      "Epoch 20/200\n",
      "8/8 [==============================] - 0s 589us/step - loss: 0.0525\n",
      "Epoch 21/200\n",
      "8/8 [==============================] - 0s 778us/step - loss: 0.0504\n",
      "Epoch 22/200\n",
      "8/8 [==============================] - 0s 702us/step - loss: 0.0483\n",
      "Epoch 23/200\n",
      "8/8 [==============================] - 0s 360us/step - loss: 0.0460\n",
      "Epoch 24/200\n",
      "8/8 [==============================] - 0s 324us/step - loss: 0.0436\n",
      "Epoch 25/200\n",
      "8/8 [==============================] - 0s 495us/step - loss: 0.0411\n",
      "Epoch 26/200\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.0385\n",
      "Epoch 27/200\n",
      "8/8 [==============================] - 0s 371us/step - loss: 0.0358\n",
      "Epoch 28/200\n",
      "8/8 [==============================] - 0s 379us/step - loss: 0.0330\n",
      "Epoch 29/200\n",
      "8/8 [==============================] - 0s 456us/step - loss: 0.0300\n",
      "Epoch 30/200\n",
      "8/8 [==============================] - 0s 718us/step - loss: 0.0295\n",
      "Epoch 31/200\n",
      "8/8 [==============================] - 0s 355us/step - loss: 0.0415\n",
      "Epoch 32/200\n",
      "8/8 [==============================] - 0s 430us/step - loss: 0.0245\n",
      "Epoch 33/200\n",
      "8/8 [==============================] - 0s 426us/step - loss: 0.0271\n",
      "Epoch 34/200\n",
      "8/8 [==============================] - 0s 504us/step - loss: 0.0342\n",
      "Epoch 35/200\n",
      "8/8 [==============================] - 0s 477us/step - loss: 0.0365\n",
      "Epoch 36/200\n",
      "8/8 [==============================] - 0s 538us/step - loss: 0.0217\n",
      "Epoch 37/200\n",
      "8/8 [==============================] - 0s 993us/step - loss: 0.0217\n",
      "Epoch 38/200\n",
      "8/8 [==============================] - 0s 397us/step - loss: 0.0178\n",
      "Epoch 39/200\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.0177\n",
      "Epoch 40/200\n",
      "8/8 [==============================] - 0s 645us/step - loss: 0.0282\n",
      "Epoch 41/200\n",
      "8/8 [==============================] - 0s 565us/step - loss: 0.0226\n",
      "Epoch 42/200\n",
      "8/8 [==============================] - 0s 442us/step - loss: 0.0292\n",
      "Epoch 43/200\n",
      "8/8 [==============================] - 0s 369us/step - loss: 0.0197\n",
      "Epoch 44/200\n",
      "8/8 [==============================] - 0s 392us/step - loss: 0.0303\n",
      "Epoch 45/200\n",
      "8/8 [==============================] - 0s 451us/step - loss: 0.0193\n",
      "Epoch 46/200\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.0198\n",
      "Epoch 47/200\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0314\n",
      "Epoch 48/200\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.0269\n",
      "Epoch 49/200\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.0206\n",
      "Epoch 50/200\n",
      "8/8 [==============================] - 0s 770us/step - loss: 0.0282\n",
      "Epoch 51/200\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0185\n",
      "Epoch 52/200\n",
      "8/8 [==============================] - 0s 578us/step - loss: 0.0294\n",
      "Epoch 53/200\n",
      "8/8 [==============================] - 0s 755us/step - loss: 0.0174\n",
      "Epoch 54/200\n",
      "8/8 [==============================] - 0s 538us/step - loss: 0.0189\n",
      "Epoch 55/200\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.0240\n",
      "Epoch 56/200\n",
      "8/8 [==============================] - 0s 441us/step - loss: 0.0239\n",
      "Epoch 57/200\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.0252\n",
      "Epoch 58/200\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.0218\n",
      "Epoch 59/200\n",
      "8/8 [==============================] - 0s 733us/step - loss: 0.0264\n",
      "Epoch 60/200\n",
      "8/8 [==============================] - 0s 576us/step - loss: 0.0199\n",
      "Epoch 61/200\n",
      "8/8 [==============================] - 0s 481us/step - loss: 0.0276\n",
      "Epoch 62/200\n",
      "8/8 [==============================] - 0s 682us/step - loss: 0.0197\n",
      "Epoch 63/200\n",
      "8/8 [==============================] - 0s 619us/step - loss: 0.0180\n",
      "Epoch 64/200\n",
      "8/8 [==============================] - 0s 654us/step - loss: 0.0312\n",
      "Epoch 65/200\n",
      "8/8 [==============================] - 0s 443us/step - loss: 0.0249\n",
      "Epoch 66/200\n",
      "8/8 [==============================] - 0s 986us/step - loss: 0.0214\n",
      "Epoch 67/200\n",
      "8/8 [==============================] - 0s 515us/step - loss: 0.0262\n",
      "Epoch 68/200\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0196\n",
      "Epoch 69/200\n",
      "8/8 [==============================] - 0s 813us/step - loss: 0.0274\n",
      "Epoch 70/200\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.0179\n",
      "Epoch 71/200\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0287\n",
      "Epoch 72/200\n",
      "8/8 [==============================] - 0s 375us/step - loss: 0.0177\n",
      "Epoch 73/200\n",
      "8/8 [==============================] - 0s 591us/step - loss: 0.0181\n",
      "Restoring model weights from the end of the best epoch\n",
      "Epoch 00073: early stopping\n",
      "best epoch =  53\n",
      "smallest loss = 0.017406456172466278\n"
     ]
    }
   ],
   "source": [
    "sgd = tf.keras.optimizers.RMSprop(learning_rate=0.01)\n",
    "model.compile(loss='mean_absolute_error',optimizer=sgd)\n",
    "\n",
    "historyData = model.fit(xarray,df.y3,epochs=200,callbacks=[es])\n",
    "\n",
    "loss_hist = historyData.history['loss']\n",
    "#The above line will return a dictionary, access it's info like this:\n",
    "best_epoch = np.argmin(historyData.history['loss']) + 1\n",
    "print ('best epoch = ', best_epoch)\n",
    "print('smallest loss =', np.min(loss_hist))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "8/8 [==============================] - 1s 111ms/step - loss: 0.0189\n",
      "\n",
      "Epoch 00001: loss improved from inf to 0.01893, saving model to best_model.SB\n",
      "Epoch 2/200\n",
      "8/8 [==============================] - 0s 311us/step - loss: 0.0178\n",
      "\n",
      "Epoch 00002: loss improved from 0.01893 to 0.01779, saving model to best_model.SB\n",
      "Epoch 3/200\n",
      "8/8 [==============================] - 0s 602us/step - loss: 0.0183\n",
      "\n",
      "Epoch 00003: loss did not improve from 0.01779\n",
      "Epoch 4/200\n",
      "8/8 [==============================] - 0s 353us/step - loss: 0.0169\n",
      "\n",
      "Epoch 00004: loss improved from 0.01779 to 0.01694, saving model to best_model.SB\n",
      "Epoch 5/200\n",
      "8/8 [==============================] - 0s 791us/step - loss: 0.0169\n",
      "\n",
      "Epoch 00005: loss improved from 0.01694 to 0.01692, saving model to best_model.SB\n",
      "Epoch 6/200\n",
      "8/8 [==============================] - 0s 358us/step - loss: 0.0180\n",
      "\n",
      "Epoch 00006: loss did not improve from 0.01692\n",
      "Epoch 7/200\n",
      "8/8 [==============================] - 0s 386us/step - loss: 0.0173\n",
      "\n",
      "Epoch 00007: loss did not improve from 0.01692\n",
      "Epoch 8/200\n",
      "8/8 [==============================] - 0s 726us/step - loss: 0.0171\n",
      "\n",
      "Epoch 00008: loss did not improve from 0.01692\n",
      "Epoch 9/200\n",
      "8/8 [==============================] - 0s 651us/step - loss: 0.0170\n",
      "\n",
      "Epoch 00009: loss did not improve from 0.01692\n",
      "Epoch 10/200\n",
      "8/8 [==============================] - 0s 635us/step - loss: 0.0168\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Applications/Software/Anaconda/anaconda3/lib/python3.6/site-packages/keras/engine/saving.py:165: UserWarning: TensorFlow optimizers do not make it possible to access optimizer attributes or optimizer state after instantiation. As a result, we cannot save the optimizer as part of the model save file.You will have to compile your model again after loading it. Prefer using a Keras optimizer instead (see keras.io/optimizers).\n",
      "  'TensorFlow optimizers do not '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00010: loss improved from 0.01692 to 0.01685, saving model to best_model.SB\n",
      "Epoch 11/200\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.0167\n",
      "\n",
      "Epoch 00011: loss improved from 0.01685 to 0.01673, saving model to best_model.SB\n",
      "Epoch 12/200\n",
      "8/8 [==============================] - 0s 708us/step - loss: 0.0169\n",
      "\n",
      "Epoch 00012: loss did not improve from 0.01673\n",
      "Epoch 13/200\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.0182\n",
      "\n",
      "Epoch 00013: loss did not improve from 0.01673\n",
      "Epoch 14/200\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.0173\n",
      "\n",
      "Epoch 00014: loss did not improve from 0.01673\n",
      "Epoch 15/200\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.0170\n",
      "\n",
      "Epoch 00015: loss did not improve from 0.01673\n",
      "Epoch 16/200\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.0170\n",
      "\n",
      "Epoch 00016: loss did not improve from 0.01673\n",
      "Epoch 17/200\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.0173\n",
      "\n",
      "Epoch 00017: loss did not improve from 0.01673\n",
      "Epoch 18/200\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.0173\n",
      "\n",
      "Epoch 00018: loss did not improve from 0.01673\n",
      "Epoch 19/200\n",
      "8/8 [==============================] - 0s 835us/step - loss: 0.0170\n",
      "\n",
      "Epoch 00019: loss did not improve from 0.01673\n",
      "Epoch 20/200\n",
      "8/8 [==============================] - 0s 769us/step - loss: 0.0169\n",
      "\n",
      "Epoch 00020: loss did not improve from 0.01673\n",
      "Epoch 21/200\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0168\n",
      "\n",
      "Epoch 00021: loss did not improve from 0.01673\n",
      "Epoch 22/200\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.0169\n",
      "\n",
      "Epoch 00022: loss did not improve from 0.01673\n",
      "Epoch 23/200\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.0177\n",
      "\n",
      "Epoch 00023: loss did not improve from 0.01673\n",
      "Epoch 24/200\n",
      "8/8 [==============================] - 0s 833us/step - loss: 0.0173\n",
      "\n",
      "Epoch 00024: loss did not improve from 0.01673\n",
      "Epoch 25/200\n",
      "8/8 [==============================] - 0s 882us/step - loss: 0.0169\n",
      "\n",
      "Epoch 00025: loss did not improve from 0.01673\n",
      "Epoch 26/200\n",
      "8/8 [==============================] - 0s 643us/step - loss: 0.0168\n",
      "\n",
      "Epoch 00026: loss did not improve from 0.01673\n",
      "Epoch 27/200\n",
      "8/8 [==============================] - 0s 874us/step - loss: 0.0167\n",
      "\n",
      "Epoch 00027: loss did not improve from 0.01673\n",
      "Epoch 28/200\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0167\n",
      "\n",
      "Epoch 00028: loss did not improve from 0.01673\n",
      "Epoch 29/200\n",
      "8/8 [==============================] - 0s 822us/step - loss: 0.0180\n",
      "\n",
      "Epoch 00029: loss did not improve from 0.01673\n",
      "Epoch 30/200\n",
      "8/8 [==============================] - 0s 810us/step - loss: 0.0171\n",
      "\n",
      "Epoch 00030: loss did not improve from 0.01673\n",
      "Epoch 31/200\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.0170\n",
      "Restoring model weights from the end of the best epoch\n",
      "\n",
      "Epoch 00031: loss did not improve from 0.01673\n",
      "Epoch 00031: early stopping\n",
      "best epoch =  11\n",
      "smallest loss = 0.01672607660293579\n"
     ]
    }
   ],
   "source": [
    "sgd = tf.keras.optimizers.RMSprop(learning_rate=0.001)\n",
    "model.compile(loss='mean_absolute_error',optimizer=sgd)\n",
    "\n",
    "historyData = model.fit(xarray,df.y3,epochs=200,callbacks=[es,mc])\n",
    "\n",
    "loss_hist = historyData.history['loss']\n",
    "#The above line will return a dictionary, access it's info like this:\n",
    "best_epoch = np.argmin(historyData.history['loss']) + 1\n",
    "print ('best epoch = ', best_epoch)\n",
    "print('smallest loss =', np.min(loss_hist))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.564119  ]\n",
      " [-0.45188004]\n",
      " [-0.5696211 ]]\n",
      "w01 =  1.564119 w02 =  -0.45188004 w03 =  -0.5696211\n",
      "[-0.52396977]\n",
      "b1 =  [-0.52396977]\n",
      "[[0.9981894]]\n",
      "w12 =  0.9981894\n",
      "[-0.11985482]\n",
      "b2 =  [-0.11985482]\n",
      "[[0.3756329]]\n",
      "w23 =  0.3756329\n",
      "[1.0126067]\n",
      "b3 =  [1.0126067]\n",
      "x01/20.2,  x02/14.5,   x03/308.0,  y3/32.4,  a3:\n",
      "0.9900990099009901 0.896551724137931 1.009090909090909 0.9558346964599856 [[0.9853929]]\n",
      "0.9900990099009901 1.0 1.0 0.9968828122588808 [[0.971232]]\n",
      "0.9900990099009901 1.0551724137931036 0.9935064935064936 0.9721922162896206 [[0.96427524]]\n",
      "1.0 0.896551724137931 1.009090909090909 0.9539829017622912 [[0.99082077]]\n",
      "0.9900990099009901 1.0 1.0 1.003055461251196 [[0.971232]]\n",
      "1.0 1.0551724137931036 0.9935064935064936 0.9691058917934631 [[0.96932137]]\n",
      "1.188118811881188 0.896551724137931 1.009090909090909 1.0984228881824636 [[1.1004896]]\n",
      "1.7821782178217822 1.0 1.0 1.4320545662170918 [[1.4333022]]\n",
      "  \n",
      "x01,  x02,   x03,  y3,  a3*32.4:\n",
      "20.0 13.0 310.8 30.969044165303533 [[31.926733]]\n",
      "20.0 14.5 308.0 32.29900311718774 [[31.467918]]\n",
      "20.0 15.3 306.0 31.499027807783705 [[31.24252]]\n",
      "20.2 13.0 310.8 30.909046017098234 [[32.102592]]\n",
      "20.0 14.5 308.0 32.498996944538746 [[31.467918]]\n",
      "20.2 15.3 306.0 31.3990308941082 [[31.406013]]\n",
      "23.999999999999996 13.0 310.8 35.58890157711182 [[35.655865]]\n",
      "36.0 14.5 308.0 46.398567945433776 [[46.43899]]\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "#For results of training network:\n",
    "\n",
    "#keras.layer.get_weights() function retrieves weight values\n",
    "first_layer_weights = model.layers[0].get_weights()[0]\n",
    "w01 = first_layer_weights[0][0]\n",
    "w02 = first_layer_weights[1][0]\n",
    "w03 = first_layer_weights[2][0]\n",
    "first_layer_bias  = model.layers[0].get_weights()[1]\n",
    "b1 = first_layer_bias\n",
    "second_layer_weights = model.layers[1].get_weights()[0]\n",
    "w12 = second_layer_weights[0][0]\n",
    "second_layer_bias  = model.layers[1].get_weights()[1]\n",
    "b2 = second_layer_bias\n",
    "third_layer_weights = model.layers[2].get_weights()[0]\n",
    "w23 = third_layer_weights[0][0]\n",
    "third_layer_bias  = model.layers[2].get_weights()[1]\n",
    "b3 = third_layer_bias\n",
    "\n",
    "#print weights and biases\n",
    "print (first_layer_weights)\n",
    "print ('w01 = ', w01, 'w02 = ', w02, 'w03 = ', w03)\n",
    "print (first_layer_bias)\n",
    "print ('b1 = ', b1)\n",
    "print (second_layer_weights)\n",
    "print ('w12 = ', w12)\n",
    "print (second_layer_bias)\n",
    "print ('b2 = ', b2)\n",
    "print (third_layer_weights)\n",
    "print ('w23 = ', w23)\n",
    "print (third_layer_bias)\n",
    "print ('b3 = ', b3)\n",
    "\n",
    "#use model.predict() function to print model predictions for data conditions\n",
    "xarray= np.array(xdata)\n",
    "print ('x01/20.2,  x02/14.5,   x03/308.0,  y3/32.4,  a3:')\n",
    "test = []\n",
    "for i in range(0,8): \n",
    "    test = [[xarray[i][0], xarray[i][1], xarray[i][2]]]\n",
    "    testarray = np.array(test)\n",
    "    a3 = model.predict(testarray)\n",
    "    print (xarray[i][0], xarray[i][1], xarray[i][2], df.y3[i], a3)\n",
    "print('  ')\n",
    "print ('x01,  x02,   x03,  y3,  a3*32.4:')\n",
    "for i in range(0,8): \n",
    "    test = [[xarray[i][0], xarray[i][1], xarray[i][2]]]\n",
    "    testarray = np.array(test)\n",
    "    a3 = model.predict(testarray)\n",
    "    print (xarray[i][0]*20.2, xarray[i][1]*14.5, xarray[i][2]*308.0, df.y3[i]*32.4, a3*32.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictedvals = np.array([32.4*model.predict(np.array([xarray[i]]))[0][0] for i in range(8)])\n",
    "actualvals = np.array(32.4*df.y3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "fig,ax = plt.subplots()\n",
    "ax.scatter(actualvals,predictedvals)\n",
    "ax.set_xscale('log')\n",
    "ax.set_yscale('log')\n",
    "ax.set_xlabel(\"Actual values\")\n",
    "ax.set_ylabel(\"Predicted values\")\n",
    "ax.set_title(\"Comparison of predicted values with actual values\")\n",
    "ax.grid()\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([31.92673087, 31.4679167 , 31.24251781, 32.1025928 , 31.4679167 ,\n",
       "       31.40601239, 35.65586357, 46.43899012])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictedvals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([30.96904417, 32.29900312, 31.49902781, 30.90904602, 32.49899694,\n",
       "       31.39903089, 35.58890158, 46.39856795])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "actualvals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
