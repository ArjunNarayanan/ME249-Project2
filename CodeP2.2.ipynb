{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    0.990099\n",
      "1    0.990099\n",
      "2    0.990099\n",
      "3    1.000000\n",
      "4    0.990099\n",
      "5    1.000000\n",
      "6    1.188119\n",
      "7    1.782178\n",
      "Name: x01, dtype: float64 0    0.896552\n",
      "1    1.000000\n",
      "2    1.055172\n",
      "3    0.896552\n",
      "4    1.000000\n",
      "5    1.055172\n",
      "6    0.896552\n",
      "7    1.000000\n",
      "Name: x02, dtype: float64 0    1.009091\n",
      "1    1.000000\n",
      "2    0.993506\n",
      "3    1.009091\n",
      "4    1.000000\n",
      "5    0.993506\n",
      "6    1.009091\n",
      "7    1.000000\n",
      "Name: x03, dtype: float64 0    0.955835\n",
      "1    0.996883\n",
      "2    0.972192\n",
      "3    0.953983\n",
      "4    1.003055\n",
      "5    0.969106\n",
      "6    1.098423\n",
      "7    1.432055\n",
      "Name: y3, dtype: float64\n",
      "[[0.9900990099009901, 0.896551724137931, 1.009090909090909], [0.9900990099009901, 1.0, 1.0], [0.9900990099009901, 1.0551724137931036, 0.9935064935064936], [1.0, 0.896551724137931, 1.009090909090909], [0.9900990099009901, 1.0, 1.0], [1.0, 1.0551724137931036, 0.9935064935064936], [1.188118811881188, 0.896551724137931, 1.009090909090909], [1.7821782178217822, 1.0, 1.0]]\n",
      "[[0.99009901 0.89655172 1.00909091]\n",
      " [0.99009901 1.         1.        ]\n",
      " [0.99009901 1.05517241 0.99350649]\n",
      " [1.         0.89655172 1.00909091]\n",
      " [0.99009901 1.         1.        ]\n",
      " [1.         1.05517241 0.99350649]\n",
      " [1.18811881 0.89655172 1.00909091]\n",
      " [1.78217822 1.         1.        ]]\n"
     ]
    }
   ],
   "source": [
    "'''>>>>> start CodeP2.2\n",
    "    V.P. Carey ME249, Spring 2021\n",
    "\n",
    "Intro to Neural Network Modeling \n",
    "Keras model for comparison with first principles model'''\n",
    "\n",
    "#import useful packages\n",
    "import keras\n",
    "import pandas as pd\n",
    "from keras.models import Sequential\n",
    "import numpy as np\n",
    "import keras.backend as kb\n",
    "import tensorflow as tf\n",
    "\n",
    "#the follwoing 2 lines are only needed for Mac OS machines\n",
    "# import os\n",
    "# os.environ['KMP_DUPLICATE_LIB_OK']='True'\n",
    "\n",
    "#raw data in dictionary form x01, x02, x03, y3\n",
    "my_dict = { \n",
    "    'x01' : [20., 20., 20., 20.2, 20., 20.2, 24.0, 36.],\n",
    "    'x02' : [13., 14.5, 15.3, 13., 14.5, 15.3, 13., 14.5],\n",
    "    'x03' : [310.8, 308.0, 306.0, 310.8, 308.0, 306.0, 310.8, 308.0],\n",
    "    'y3' : [30.97, 32.3, 31.5, 30.91, 32.5, 31.4, 35.59, 46.4]\n",
    "}\n",
    "#normalized inputs in array\n",
    "xdata = []\n",
    "xdata = [[20./20.2, 13.0/14.5, 310.8/308.0], [20./20.2, 14.5/14.5, 308.0/308.0]] \n",
    "xdata.append([20./20.2, 15.3/14.5, 306.0/308.0])\n",
    "xdata.append([20.2/20.2, 13.0/14.5, 310.8/308.0]) \n",
    "xdata.append([20./20.2, 14.5/14.5, 308.0/308.0]) \n",
    "xdata.append([20.2/20.2, 15.3/14.5, 306.0/308.0]) \n",
    "xdata.append([24./20.2, 13.0/14.5, 310.8/308.0]) \n",
    "xdata.append([36./20.2, 14.5/14.5, 308.0/308.0]) \n",
    "#data frame\n",
    "df = pd.DataFrame(my_dict)\n",
    "#devide by the median to normalize \n",
    "df.x01= df.x01/20.2\n",
    "df.x02= df.x02/14.5\n",
    "df.x03= df.x03/308.0\n",
    "#normalize output array\n",
    "df.y3= df.y3/32.401\n",
    "df.head\n",
    "print (df.x01, df.x02, df.x03, df.y3)\n",
    "\n",
    "xarray= np.array(xdata)\n",
    "print (xdata)\n",
    "print (xarray)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define model\n",
    "\n",
    "#As seen below, we have created three dense layers each with just one neuron. \n",
    "#A dense layer is a layer in neural network that’s fully connected. \n",
    "#In other words, all the neurons in one layer are connected to all other neurons in the next layer.\n",
    "#In the first layer, we need to provide the input shape, which is 3 in this case. \n",
    "#The activation function we have chosen is ReLU, which stands for rectified linear unit.\n",
    "\n",
    "from keras import backend as K\n",
    "#initialize weights with values between -0.2 and 1.2\n",
    "initializer = tf.keras.initializers.RandomUniform(minval= -0.2, maxval=1.2)\n",
    "\n",
    "# define three layer model with one neuron in each layer\n",
    "model = keras.Sequential([\n",
    "    keras.layers.Dense(1, activation=K.elu, input_shape=[3]),\n",
    "    keras.layers.Dense(1, activation=K.elu),\n",
    "    keras.layers.Dense(1, activation=K.elu)\n",
    "  ])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We’re using RMSprop as our optimizer here. RMSprop stands for Root Mean Square Propagation. \n",
    "#It’s one of the most popular gradient descent optimization algorithms for deep learning networks. \n",
    "#RMSprop is an optimizer that’s reliable and fast.\n",
    "#We’re compiling the mode using the model.compile function. The loss function used here \n",
    "#is mean absolute error. After the compilation of the model, we’ll use the fit method with 100 epochs.\n",
    "\n",
    "#Running model.fit successive times extends the calculation to addtional epochs.\n",
    "\n",
    "sgd = tf.keras.optimizers.RMSprop(learning_rate=0.001)\n",
    "model.compile(loss='mean_absolute_error',optimizer=sgd)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "8/8 [==============================] - 1s 93ms/step - loss: 0.0212\n",
      "Epoch 2/100\n",
      "8/8 [==============================] - 0s 413us/step - loss: 0.0191\n",
      "Epoch 3/100\n",
      "8/8 [==============================] - 0s 568us/step - loss: 0.0188\n",
      "Epoch 4/100\n",
      "8/8 [==============================] - 0s 990us/step - loss: 0.0189\n",
      "Epoch 5/100\n",
      "8/8 [==============================] - 0s 658us/step - loss: 0.0189\n",
      "Epoch 6/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.0190\n",
      "Epoch 7/100\n",
      "8/8 [==============================] - 0s 881us/step - loss: 0.0180\n",
      "Epoch 8/100\n",
      "8/8 [==============================] - 0s 446us/step - loss: 0.0180\n",
      "Epoch 9/100\n",
      "8/8 [==============================] - 0s 767us/step - loss: 0.0190\n",
      "Epoch 10/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.0179\n",
      "Epoch 11/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0190\n",
      "Epoch 12/100\n",
      "8/8 [==============================] - 0s 985us/step - loss: 0.0180\n",
      "Epoch 13/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0188\n",
      "Epoch 14/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.0181\n",
      "Epoch 15/100\n",
      "8/8 [==============================] - 0s 549us/step - loss: 0.0187\n",
      "Epoch 16/100\n",
      "8/8 [==============================] - 0s 869us/step - loss: 0.0182\n",
      "Epoch 17/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0185\n",
      "Epoch 18/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.0183\n",
      "Epoch 19/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.0183\n",
      "Epoch 20/100\n",
      "8/8 [==============================] - 0s 623us/step - loss: 0.0184\n",
      "Epoch 21/100\n",
      "8/8 [==============================] - 0s 797us/step - loss: 0.0182\n",
      "Epoch 22/100\n",
      "8/8 [==============================] - 0s 479us/step - loss: 0.0185\n",
      "Epoch 23/100\n",
      "8/8 [==============================] - 0s 999us/step - loss: 0.0180\n",
      "Epoch 24/100\n",
      "8/8 [==============================] - 0s 685us/step - loss: 0.0187\n",
      "Epoch 25/100\n",
      "8/8 [==============================] - 0s 896us/step - loss: 0.0180\n",
      "Epoch 26/100\n",
      "8/8 [==============================] - 0s 806us/step - loss: 0.0180\n",
      "Epoch 27/100\n",
      "8/8 [==============================] - 0s 580us/step - loss: 0.0187\n",
      "Epoch 28/100\n",
      "8/8 [==============================] - 0s 437us/step - loss: 0.0179\n",
      "Epoch 29/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.0180\n",
      "Epoch 30/100\n",
      "8/8 [==============================] - 0s 726us/step - loss: 0.0187\n",
      "Epoch 31/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0179\n",
      "Epoch 32/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0188\n",
      "Epoch 33/100\n",
      "8/8 [==============================] - 0s 805us/step - loss: 0.0179\n",
      "Epoch 34/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0187\n",
      "Epoch 35/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.0181\n",
      "Epoch 36/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0186\n",
      "Epoch 37/100\n",
      "8/8 [==============================] - 0s 935us/step - loss: 0.0182\n",
      "Epoch 38/100\n",
      "8/8 [==============================] - 0s 492us/step - loss: 0.0184\n",
      "Epoch 39/100\n",
      "8/8 [==============================] - 0s 558us/step - loss: 0.0183\n",
      "Epoch 40/100\n",
      "8/8 [==============================] - 0s 921us/step - loss: 0.0183\n",
      "Epoch 41/100\n",
      "8/8 [==============================] - 0s 530us/step - loss: 0.0184\n",
      "Epoch 42/100\n",
      "8/8 [==============================] - 0s 417us/step - loss: 0.0182\n",
      "Epoch 43/100\n",
      "8/8 [==============================] - 0s 441us/step - loss: 0.0185\n",
      "Epoch 44/100\n",
      "8/8 [==============================] - 0s 614us/step - loss: 0.0180\n",
      "Epoch 45/100\n",
      "8/8 [==============================] - 0s 412us/step - loss: 0.0186\n",
      "Epoch 46/100\n",
      "8/8 [==============================] - 0s 595us/step - loss: 0.0179\n",
      "Epoch 47/100\n",
      "8/8 [==============================] - 0s 460us/step - loss: 0.0180\n",
      "Epoch 48/100\n",
      "8/8 [==============================] - 0s 457us/step - loss: 0.0186\n",
      "Epoch 49/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.0179\n",
      "Epoch 50/100\n",
      "8/8 [==============================] - 0s 417us/step - loss: 0.0187\n",
      "Epoch 51/100\n",
      "8/8 [==============================] - 0s 745us/step - loss: 0.0179\n",
      "Epoch 52/100\n",
      "8/8 [==============================] - 0s 432us/step - loss: 0.0179\n",
      "Epoch 53/100\n",
      "8/8 [==============================] - 0s 436us/step - loss: 0.0188\n",
      "Epoch 54/100\n",
      "8/8 [==============================] - 0s 838us/step - loss: 0.0179\n",
      "Epoch 55/100\n",
      "8/8 [==============================] - 0s 434us/step - loss: 0.0187\n",
      "Epoch 56/100\n",
      "8/8 [==============================] - 0s 376us/step - loss: 0.0180\n",
      "Epoch 57/100\n",
      "8/8 [==============================] - 0s 530us/step - loss: 0.0186\n",
      "Epoch 58/100\n",
      "8/8 [==============================] - 0s 878us/step - loss: 0.0181\n",
      "Epoch 59/100\n",
      "8/8 [==============================] - 0s 423us/step - loss: 0.0185\n",
      "Epoch 60/100\n",
      "8/8 [==============================] - 0s 443us/step - loss: 0.0182\n",
      "Epoch 61/100\n",
      "8/8 [==============================] - 0s 529us/step - loss: 0.0183\n",
      "Epoch 62/100\n",
      "8/8 [==============================] - 0s 757us/step - loss: 0.0183\n",
      "Epoch 63/100\n",
      "8/8 [==============================] - 0s 439us/step - loss: 0.0182\n",
      "Epoch 64/100\n",
      "8/8 [==============================] - 0s 506us/step - loss: 0.0184\n",
      "Epoch 65/100\n",
      "8/8 [==============================] - 0s 499us/step - loss: 0.0180\n",
      "Epoch 66/100\n",
      "8/8 [==============================] - 0s 501us/step - loss: 0.0185\n",
      "Epoch 67/100\n",
      "8/8 [==============================] - 0s 996us/step - loss: 0.0179\n",
      "Epoch 68/100\n",
      "8/8 [==============================] - 0s 438us/step - loss: 0.0186\n",
      "Epoch 69/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.0179\n",
      "Epoch 70/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.0179\n",
      "Epoch 71/100\n",
      "8/8 [==============================] - 0s 905us/step - loss: 0.0187\n",
      "Epoch 72/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0178\n",
      "Epoch 73/100\n",
      "8/8 [==============================] - 0s 430us/step - loss: 0.0179\n",
      "Epoch 74/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0187\n",
      "Epoch 75/100\n",
      "8/8 [==============================] - 0s 550us/step - loss: 0.0178\n",
      "Epoch 76/100\n",
      "8/8 [==============================] - 0s 663us/step - loss: 0.0180\n",
      "Epoch 77/100\n",
      "8/8 [==============================] - 0s 756us/step - loss: 0.0187\n",
      "Epoch 78/100\n",
      "8/8 [==============================] - 0s 425us/step - loss: 0.0185\n",
      "Epoch 79/100\n",
      "8/8 [==============================] - 0s 672us/step - loss: 0.0180\n",
      "Epoch 80/100\n",
      "8/8 [==============================] - 0s 462us/step - loss: 0.0186\n",
      "Epoch 81/100\n",
      "8/8 [==============================] - 0s 390us/step - loss: 0.0178\n",
      "Epoch 82/100\n",
      "8/8 [==============================] - 0s 484us/step - loss: 0.0187\n",
      "Epoch 83/100\n",
      "8/8 [==============================] - 0s 492us/step - loss: 0.0178\n",
      "Epoch 84/100\n",
      "8/8 [==============================] - 0s 767us/step - loss: 0.0187\n",
      "Epoch 85/100\n",
      "8/8 [==============================] - 0s 497us/step - loss: 0.0183\n",
      "Epoch 86/100\n",
      "8/8 [==============================] - 0s 548us/step - loss: 0.0182\n",
      "Epoch 87/100\n",
      "8/8 [==============================] - 0s 413us/step - loss: 0.0184\n",
      "Epoch 88/100\n",
      "8/8 [==============================] - 0s 671us/step - loss: 0.0180\n",
      "Epoch 89/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.0185\n",
      "Epoch 90/100\n",
      "8/8 [==============================] - 0s 881us/step - loss: 0.0179\n",
      "Epoch 91/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.0186\n",
      "Epoch 92/100\n",
      "8/8 [==============================] - 0s 553us/step - loss: 0.0179\n",
      "Epoch 93/100\n",
      "8/8 [==============================] - 0s 450us/step - loss: 0.0180\n",
      "Epoch 94/100\n",
      "8/8 [==============================] - 0s 669us/step - loss: 0.0186\n",
      "Epoch 95/100\n",
      "8/8 [==============================] - 0s 628us/step - loss: 0.0184\n",
      "Epoch 96/100\n",
      "8/8 [==============================] - 0s 649us/step - loss: 0.0180\n",
      "Epoch 97/100\n",
      "8/8 [==============================] - 0s 485us/step - loss: 0.0185\n",
      "Epoch 98/100\n",
      "8/8 [==============================] - 0s 434us/step - loss: 0.0178\n",
      "Epoch 99/100\n",
      "8/8 [==============================] - 0s 462us/step - loss: 0.0179\n",
      "Epoch 100/100\n",
      "8/8 [==============================] - 0s 531us/step - loss: 0.0187\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x7facaec69cc0>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#After the compilation of the model, we’ll use the fit method with 500 epochs.\n",
    "#I started with epochs value of 100 and then tested the model after training. \n",
    "#The prediction was not that good. Then I modified the number of epochs to 200 and tested the model again. \n",
    "#Accuracy had improved slightly, but figured I’d give it one more try. Finally, at 500 epochs \n",
    "#I found acceptable prediction accuracy.\n",
    "\n",
    "#The fit method takes three parameters; namely, x, y, and number of epochs. \n",
    "#During model training, if all the batches of data are seen by the model once, \n",
    "#we say that one epoch has been completed.\n",
    "\n",
    "model.fit(xarray,df.y3,epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.5546382  -0.33132634  1.3338982  -2.3136754 ]\n",
      " [-0.78900415  0.00339559 -0.04200174 -1.6776005 ]\n",
      " [-1.3649417  -0.02423315 -0.04164204 -2.0827448 ]]\n",
      "w01 =  -0.5546382 w02 =  -0.78900415 w03 =  -1.3649417\n",
      "[-0.44586802  0.4186515  -0.7936948  -1.856011  ]\n",
      "b1 =  [-0.44586802  0.4186515  -0.7936948  -1.856011  ]\n",
      "[[ 0.02345913  0.24678303  1.7479653   1.4780577 ]\n",
      " [ 0.38615236 -0.09244318  2.4384959   1.039276  ]\n",
      " [-0.50902224  1.0566386  -1.1039048  -1.5330113 ]\n",
      " [ 0.22729892  0.43058276  1.465031    0.561297  ]]\n",
      "w12 =  0.023459125\n",
      "[ 0.6620204   0.29805145 -1.06413    -0.7666336 ]\n",
      "b2 =  [ 0.6620204   0.29805145 -1.06413    -0.7666336 ]\n",
      "[[-0.23016934]\n",
      " [ 0.26560754]\n",
      " [ 0.3100079 ]\n",
      " [-1.2639132 ]]\n",
      "w23 =  -0.23016934\n",
      "[0.09667291]\n",
      "b3 =  [0.09667291]\n",
      "x01/20.2,  x02/14.5,   x03/308.0,  y3/32.4,  a3:\n",
      "0.9900990099009901 0.896551724137931 1.009090909090909 0.9558346964599856 [[0.9871613]]\n",
      "0.9900990099009901 1.0 1.0 0.9968828122588808 [[0.98523796]]\n",
      "0.9900990099009901 1.0551724137931036 0.9935064935064936 0.9721922162896206 [[0.984238]]\n",
      "1.0 0.896551724137931 1.009090909090909 0.9539829017622912 [[0.9937461]]\n",
      "0.9900990099009901 1.0 1.0 1.003055461251196 [[0.98523796]]\n",
      "1.0 1.0551724137931036 0.9935064935064936 0.9691058917934631 [[0.99082655]]\n",
      "1.188118811881188 0.896551724137931 1.009090909090909 1.0984228881824636 [[1.1150246]]\n",
      "1.7821782178217822 1.0 1.0 1.4320545662170918 [[1.4496303]]\n",
      "  \n",
      "x01,  x02,   x03,  y3,  a3*32.4:\n",
      "20.0 13.0 310.8 30.969044165303533 [[31.984028]]\n",
      "20.0 14.5 308.0 32.29900311718774 [[31.921711]]\n",
      "20.0 15.3 306.0 31.499027807783705 [[31.889313]]\n",
      "20.2 13.0 310.8 30.909046017098234 [[32.197376]]\n",
      "20.0 14.5 308.0 32.498996944538746 [[31.921711]]\n",
      "20.2 15.3 306.0 31.3990308941082 [[32.102783]]\n",
      "23.999999999999996 13.0 310.8 35.58890157711182 [[36.126797]]\n",
      "36.0 14.5 308.0 46.398567945433776 [[46.96802]]\n"
     ]
    }
   ],
   "source": [
    "# from __future__ import print_function\n",
    "#For results of training network:\n",
    "\n",
    "#keras.layer.get_weights() function retrieves weight values\n",
    "first_layer_weights = model.layers[0].get_weights()[0]\n",
    "w01 = first_layer_weights[0][0]\n",
    "w02 = first_layer_weights[1][0]\n",
    "w03 = first_layer_weights[2][0]\n",
    "first_layer_bias  = model.layers[0].get_weights()[1]\n",
    "b1 = first_layer_bias\n",
    "second_layer_weights = model.layers[1].get_weights()[0]\n",
    "w12 = second_layer_weights[0][0]\n",
    "second_layer_bias  = model.layers[1].get_weights()[1]\n",
    "b2 = second_layer_bias\n",
    "third_layer_weights = model.layers[2].get_weights()[0]\n",
    "w23 = third_layer_weights[0][0]\n",
    "third_layer_bias  = model.layers[2].get_weights()[1]\n",
    "b3 = third_layer_bias\n",
    "\n",
    "#print weights and biases\n",
    "print (first_layer_weights)\n",
    "print ('w01 = ', w01, 'w02 = ', w02, 'w03 = ', w03)\n",
    "print (first_layer_bias)\n",
    "print ('b1 = ', b1)\n",
    "print (second_layer_weights)\n",
    "print ('w12 = ', w12)\n",
    "print (second_layer_bias)\n",
    "print ('b2 = ', b2)\n",
    "print (third_layer_weights)\n",
    "print ('w23 = ', w23)\n",
    "print (third_layer_bias)\n",
    "print ('b3 = ', b3)\n",
    "\n",
    "#use model.predict() function to print model predictions for data conditions\n",
    "xarray= np.array(xdata)\n",
    "print ('x01/20.2,  x02/14.5,   x03/308.0,  y3/32.4,  a3:')\n",
    "test = []\n",
    "for i in range(0,8): \n",
    "    test = [[xarray[i][0], xarray[i][1], xarray[i][2]]]\n",
    "    testarray = np.array(test)\n",
    "    a3 = model.predict(testarray)\n",
    "    print (xarray[i][0], xarray[i][1], xarray[i][2], df.y3[i], a3)\n",
    "print('  ')\n",
    "print ('x01,  x02,   x03,  y3,  a3*32.4:')\n",
    "for i in range(0,8): \n",
    "    test = [[xarray[i][0], xarray[i][1], xarray[i][2]]]\n",
    "    testarray = np.array(test)\n",
    "    a3 = model.predict(testarray)\n",
    "    print (xarray[i][0]*20.2, xarray[i][1]*14.5, xarray[i][2]*308.0, df.y3[i]*32.4, a3*32.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<keras.layers.core.Dense at 0x7facacd94630>,\n",
       " <keras.layers.core.Dense at 0x7facacd7ef98>,\n",
       " <keras.layers.core.Dense at 0x7facacd7ea90>]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
