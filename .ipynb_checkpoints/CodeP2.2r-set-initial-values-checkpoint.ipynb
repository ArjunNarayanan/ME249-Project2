{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    0.990099\n",
      "1    0.990099\n",
      "2    0.990099\n",
      "3    1.000000\n",
      "4    0.990099\n",
      "5    1.000000\n",
      "6    1.188119\n",
      "7    1.782178\n",
      "Name: x01, dtype: float64 0    0.896552\n",
      "1    1.000000\n",
      "2    1.055172\n",
      "3    0.896552\n",
      "4    1.000000\n",
      "5    1.055172\n",
      "6    0.896552\n",
      "7    1.000000\n",
      "Name: x02, dtype: float64 0    1.009091\n",
      "1    1.000000\n",
      "2    0.993506\n",
      "3    1.009091\n",
      "4    1.000000\n",
      "5    0.993506\n",
      "6    1.009091\n",
      "7    1.000000\n",
      "Name: x03, dtype: float64 0    0.955835\n",
      "1    0.996883\n",
      "2    0.972192\n",
      "3    0.953983\n",
      "4    1.003055\n",
      "5    0.969106\n",
      "6    1.098423\n",
      "7    1.432055\n",
      "Name: y3, dtype: float64\n",
      "[[0.9900990099009901, 0.896551724137931, 1.009090909090909], [0.9900990099009901, 1.0, 1.0], [0.9900990099009901, 1.0551724137931036, 0.9935064935064936], [1.0, 0.896551724137931, 1.009090909090909], [0.9900990099009901, 1.0, 1.0], [1.0, 1.0551724137931036, 0.9935064935064936], [1.188118811881188, 0.896551724137931, 1.009090909090909], [1.7821782178217822, 1.0, 1.0]]\n",
      "[[0.99009901 0.89655172 1.00909091]\n",
      " [0.99009901 1.         1.        ]\n",
      " [0.99009901 1.05517241 0.99350649]\n",
      " [1.         0.89655172 1.00909091]\n",
      " [0.99009901 1.         1.        ]\n",
      " [1.         1.05517241 0.99350649]\n",
      " [1.18811881 0.89655172 1.00909091]\n",
      " [1.78217822 1.         1.        ]]\n"
     ]
    }
   ],
   "source": [
    "'''>>>>> start CodeP2.2\n",
    "    V.P. Carey ME249, Spring 2021\n",
    "\n",
    "Intro to Neural Network Modeling \n",
    "Keras model for comparison with first principles model'''\n",
    "\n",
    "#import useful packages\n",
    "import keras\n",
    "import pandas as pd\n",
    "from keras.models import Sequential\n",
    "import numpy as np\n",
    "import keras.backend as kb\n",
    "import tensorflow as tf\n",
    "\n",
    "#the follwoing 2 lines are only needed for Mac OS machines\n",
    "# import os\n",
    "# os.environ['KMP_DUPLICATE_LIB_OK']='True'\n",
    "\n",
    "#raw data in dictionary form x01, x02, x03, y3\n",
    "my_dict = { \n",
    "    'x01' : [20., 20., 20., 20.2, 20., 20.2, 24.0, 36.],\n",
    "    'x02' : [13., 14.5, 15.3, 13., 14.5, 15.3, 13., 14.5],\n",
    "    'x03' : [310.8, 308.0, 306.0, 310.8, 308.0, 306.0, 310.8, 308.0],\n",
    "    'y3' : [30.97, 32.3, 31.5, 30.91, 32.5, 31.4, 35.59, 46.4]\n",
    "}\n",
    "#normalized inputs in array\n",
    "xdata = []\n",
    "xdata = [[20./20.2, 13.0/14.5, 310.8/308.0], [20./20.2, 14.5/14.5, 308.0/308.0]] \n",
    "xdata.append([20./20.2, 15.3/14.5, 306.0/308.0])\n",
    "xdata.append([20.2/20.2, 13.0/14.5, 310.8/308.0]) \n",
    "xdata.append([20./20.2, 14.5/14.5, 308.0/308.0]) \n",
    "xdata.append([20.2/20.2, 15.3/14.5, 306.0/308.0]) \n",
    "xdata.append([24./20.2, 13.0/14.5, 310.8/308.0]) \n",
    "xdata.append([36./20.2, 14.5/14.5, 308.0/308.0]) \n",
    "#data frame\n",
    "df = pd.DataFrame(my_dict)\n",
    "#devide by the median to normalize \n",
    "df.x01= df.x01/20.2\n",
    "df.x02= df.x02/14.5\n",
    "df.x03= df.x03/308.0\n",
    "#normalize output array\n",
    "df.y3= df.y3/32.401\n",
    "df.head\n",
    "print (df.x01, df.x02, df.x03, df.y3)\n",
    "\n",
    "xarray= np.array(xdata)\n",
    "print (xdata)\n",
    "print (xarray)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define model\n",
    "\n",
    "#As seen below, we have created three dense layers each with just one neuron. \n",
    "#A dense layer is a layer in neural network that’s fully connected. \n",
    "#In other words, all the neurons in one layer are connected to all other neurons in the next layer.\n",
    "#In the first layer, we need to provide the input shape, which is 3 in this case. \n",
    "#The activation function we have chosen is ReLU, which stands for rectified linear unit.\n",
    "\n",
    "from keras import backend as K\n",
    "#initialize weights with values between -0.2 and 1.2\n",
    "initializer = tf.keras.initializers.RandomUniform(minval= -0.2, maxval=1.2)\n",
    "\n",
    "# define three layer model with one neuron in each layer\n",
    "model = keras.Sequential([\n",
    "    keras.layers.Dense(1, activation=K.elu, input_shape=[3]),\n",
    "    keras.layers.Dense(1, activation=K.elu),\n",
    "    keras.layers.Dense(1, activation=K.elu)\n",
    "  ])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set weights in the input layer\n",
    "model.layers[0].set_weights([np.array([[1.8],[0.55],[1]]),np.array([-0.2])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set weights in first hidden layer\n",
    "model.layers[1].set_weights([np.array([[0.9]]),np.array([-0.07])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set weights in output layer\n",
    "model.layers[2].set_weights([np.array([[0.4]]),np.array([0.05])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We’re using RMSprop as our optimizer here. RMSprop stands for Root Mean Square Propagation. \n",
    "#It’s one of the most popular gradient descent optimization algorithms for deep learning networks. \n",
    "#RMSprop is an optimizer that’s reliable and fast.\n",
    "#We’re compiling the mode using the model.compile function. The loss function used here \n",
    "#is mean absolute error. After the compilation of the model, we’ll use the fit method with 100 epochs.\n",
    "\n",
    "#Running model.fit successive times extends the calculation to addtional epochs.\n",
    "\n",
    "sgd = keras.optimizers.RMSprop(learning_rate=0.1)\n",
    "model.compile(loss='mean_absolute_error',optimizer=sgd)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "#After the compilation of the model, we’ll use the fit method with 500 epochs.\n",
    "#I started with epochs value of 100 and then tested the model after training. \n",
    "#The prediction was not that good. Then I modified the number of epochs to 200 and tested the model again. \n",
    "#Accuracy had improved slightly, but figured I’d give it one more try. Finally, at 500 epochs \n",
    "#I found acceptable prediction accuracy.\n",
    "\n",
    "#The fit method takes three parameters; namely, x, y, and number of epochs. \n",
    "#During model training, if all the batches of data are seen by the model once, \n",
    "#we say that one epoch has been completed.\n",
    "\n",
    "# Add an early stopping callback\n",
    "es = keras.callbacks.EarlyStopping(\n",
    "    monitor='loss', \n",
    "    mode='min', \n",
    "    patience = 20, \n",
    "    restore_best_weights = True, \n",
    "    verbose=1)\n",
    "# Add a checkpoint where loss is minimum, and save that model\n",
    "mc = keras.callbacks.ModelCheckpoint('best_model.SB', monitor='loss', \n",
    "                     mode='min',  verbose=1, save_best_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "8/8 [==============================] - 1s 108ms/step - loss: 0.1794\n",
      "\n",
      "Epoch 00001: loss improved from inf to 0.17939, saving model to best_model.SB\n",
      "Epoch 2/100\n",
      "8/8 [==============================] - 0s 365us/step - loss: 1.2273\n",
      "\n",
      "Epoch 00002: loss did not improve from 0.17939\n",
      "Epoch 3/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.9441\n",
      "\n",
      "Epoch 00003: loss did not improve from 0.17939\n",
      "Epoch 4/100\n",
      "8/8 [==============================] - 0s 853us/step - loss: 0.4685\n",
      "\n",
      "Epoch 00004: loss did not improve from 0.17939\n",
      "Epoch 5/100\n",
      "8/8 [==============================] - 0s 939us/step - loss: 0.4644\n",
      "\n",
      "Epoch 00005: loss did not improve from 0.17939\n",
      "Epoch 6/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.6243\n",
      "\n",
      "Epoch 00006: loss did not improve from 0.17939\n",
      "Epoch 7/100\n",
      "8/8 [==============================] - 0s 920us/step - loss: 0.1885\n",
      "\n",
      "Epoch 00007: loss did not improve from 0.17939\n",
      "Epoch 8/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.6093\n",
      "\n",
      "Epoch 00008: loss did not improve from 0.17939\n",
      "Epoch 9/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.3530\n",
      "\n",
      "Epoch 00009: loss did not improve from 0.17939\n",
      "Epoch 10/100\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.1215\n",
      "\n",
      "Epoch 00010: loss improved from 0.17939 to 0.12153, saving model to best_model.SB\n",
      "Epoch 11/100\n",
      "8/8 [==============================] - 0s 926us/step - loss: 0.4529\n",
      "\n",
      "Epoch 00011: loss did not improve from 0.12153\n",
      "Epoch 12/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1026\n",
      "\n",
      "Epoch 00012: loss improved from 0.12153 to 0.10259, saving model to best_model.SB\n",
      "Epoch 13/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.4863\n",
      "\n",
      "Epoch 00013: loss did not improve from 0.10259\n",
      "Epoch 14/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.2340\n",
      "\n",
      "Epoch 00014: loss did not improve from 0.10259\n",
      "Epoch 15/100\n",
      "8/8 [==============================] - 0s 724us/step - loss: 0.1806\n",
      "\n",
      "Epoch 00015: loss did not improve from 0.10259\n",
      "Epoch 16/100\n",
      "8/8 [==============================] - 0s 542us/step - loss: 0.3222\n",
      "\n",
      "Epoch 00016: loss did not improve from 0.10259\n",
      "Epoch 17/100\n",
      "8/8 [==============================] - 0s 647us/step - loss: 0.0504\n",
      "\n",
      "Epoch 00017: loss improved from 0.10259 to 0.05038, saving model to best_model.SB\n",
      "Epoch 18/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1906\n",
      "\n",
      "Epoch 00018: loss did not improve from 0.05038\n",
      "Epoch 19/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.2309\n",
      "\n",
      "Epoch 00019: loss did not improve from 0.05038\n",
      "Epoch 20/100\n",
      "8/8 [==============================] - 0s 446us/step - loss: 0.2828\n",
      "\n",
      "Epoch 00020: loss did not improve from 0.05038\n",
      "Epoch 21/100\n",
      "8/8 [==============================] - 0s 584us/step - loss: 0.0708\n",
      "\n",
      "Epoch 00021: loss did not improve from 0.05038\n",
      "Epoch 22/100\n",
      "8/8 [==============================] - 0s 572us/step - loss: 0.2503\n",
      "\n",
      "Epoch 00022: loss did not improve from 0.05038\n",
      "Epoch 23/100\n",
      "8/8 [==============================] - 0s 565us/step - loss: 0.1064\n",
      "\n",
      "Epoch 00023: loss did not improve from 0.05038\n",
      "Epoch 24/100\n",
      "8/8 [==============================] - 0s 672us/step - loss: 0.2197\n",
      "\n",
      "Epoch 00024: loss did not improve from 0.05038\n",
      "Epoch 25/100\n",
      "8/8 [==============================] - 0s 398us/step - loss: 0.1389\n",
      "\n",
      "Epoch 00025: loss did not improve from 0.05038\n",
      "Epoch 26/100\n",
      "8/8 [==============================] - 0s 747us/step - loss: 0.1908\n",
      "\n",
      "Epoch 00026: loss did not improve from 0.05038\n",
      "Epoch 27/100\n",
      "8/8 [==============================] - 0s 445us/step - loss: 0.1789\n",
      "\n",
      "Epoch 00027: loss did not improve from 0.05038\n",
      "Epoch 28/100\n",
      "8/8 [==============================] - 0s 584us/step - loss: 0.2721\n",
      "\n",
      "Epoch 00028: loss did not improve from 0.05038\n",
      "Epoch 29/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.0603\n",
      "\n",
      "Epoch 00029: loss did not improve from 0.05038\n",
      "Epoch 30/100\n",
      "8/8 [==============================] - 0s 703us/step - loss: 0.1601\n",
      "\n",
      "Epoch 00030: loss did not improve from 0.05038\n",
      "Epoch 31/100\n",
      "8/8 [==============================] - 0s 677us/step - loss: 0.2001\n",
      "\n",
      "Epoch 00031: loss did not improve from 0.05038\n",
      "Epoch 32/100\n",
      "8/8 [==============================] - 0s 585us/step - loss: 0.2396\n",
      "\n",
      "Epoch 00032: loss did not improve from 0.05038\n",
      "Epoch 33/100\n",
      "8/8 [==============================] - 0s 788us/step - loss: 0.0732\n",
      "\n",
      "Epoch 00033: loss did not improve from 0.05038\n",
      "Epoch 34/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.1323\n",
      "\n",
      "Epoch 00034: loss did not improve from 0.05038\n",
      "Epoch 35/100\n",
      "8/8 [==============================] - 0s 458us/step - loss: 0.2120\n",
      "\n",
      "Epoch 00035: loss did not improve from 0.05038\n",
      "Epoch 36/100\n",
      "8/8 [==============================] - 0s 805us/step - loss: 0.2081\n",
      "\n",
      "Epoch 00036: loss did not improve from 0.05038\n",
      "Epoch 37/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.0867\n",
      "Restoring model weights from the end of the best epoch\n",
      "\n",
      "Epoch 00037: loss did not improve from 0.05038\n",
      "Epoch 00037: early stopping\n",
      "best epoch =  17\n",
      "smallest loss = 0.05038444697856903\n"
     ]
    }
   ],
   "source": [
    "historyData = model.fit(xarray,df.y3,epochs=100,callbacks=[es,mc])\n",
    "\n",
    "loss_hist = historyData.history['loss']\n",
    "#The above line will return a dictionary, access it's info like this:\n",
    "best_epoch = np.argmin(historyData.history['loss']) + 1\n",
    "print ('best epoch = ', best_epoch)\n",
    "print('smallest loss =', np.min(loss_hist))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "8/8 [==============================] - 1s 100ms/step - loss: 0.1906\n",
      "\n",
      "Epoch 00001: loss did not improve from 0.05038\n",
      "Epoch 2/200\n",
      "8/8 [==============================] - 0s 543us/step - loss: 0.0599\n",
      "\n",
      "Epoch 00002: loss did not improve from 0.05038\n",
      "Epoch 3/200\n",
      "8/8 [==============================] - 0s 527us/step - loss: 0.0725\n",
      "\n",
      "Epoch 00003: loss did not improve from 0.05038\n",
      "Epoch 4/200\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0443\n",
      "\n",
      "Epoch 00004: loss improved from 0.05038 to 0.04432, saving model to best_model.SB\n",
      "Epoch 5/200\n",
      "8/8 [==============================] - 0s 450us/step - loss: 0.0439\n",
      "\n",
      "Epoch 00005: loss improved from 0.04432 to 0.04387, saving model to best_model.SB\n",
      "Epoch 6/200\n",
      "8/8 [==============================] - 0s 290us/step - loss: 0.0434\n",
      "\n",
      "Epoch 00006: loss improved from 0.04387 to 0.04339, saving model to best_model.SB\n",
      "Epoch 7/200\n",
      "8/8 [==============================] - 0s 355us/step - loss: 0.0430\n",
      "\n",
      "Epoch 00007: loss improved from 0.04339 to 0.04302, saving model to best_model.SB\n",
      "Epoch 8/200\n",
      "8/8 [==============================] - 0s 878us/step - loss: 0.0438\n",
      "\n",
      "Epoch 00008: loss did not improve from 0.04302\n",
      "Epoch 9/200\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.0432\n",
      "\n",
      "Epoch 00009: loss did not improve from 0.04302\n",
      "Epoch 10/200\n",
      "8/8 [==============================] - 0s 787us/step - loss: 0.0426\n",
      "\n",
      "Epoch 00010: loss improved from 0.04302 to 0.04262, saving model to best_model.SB\n",
      "Epoch 11/200\n",
      "8/8 [==============================] - 0s 691us/step - loss: 0.0420\n",
      "\n",
      "Epoch 00011: loss improved from 0.04262 to 0.04199, saving model to best_model.SB\n",
      "Epoch 12/200\n",
      "8/8 [==============================] - 0s 301us/step - loss: 0.0423\n",
      "\n",
      "Epoch 00012: loss did not improve from 0.04199\n",
      "Epoch 13/200\n",
      "8/8 [==============================] - 0s 375us/step - loss: 0.0425\n",
      "\n",
      "Epoch 00013: loss did not improve from 0.04199\n",
      "Epoch 14/200\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.0417\n",
      "\n",
      "Epoch 00014: loss improved from 0.04199 to 0.04174, saving model to best_model.SB\n",
      "Epoch 15/200\n",
      "8/8 [==============================] - 0s 689us/step - loss: 0.0410\n",
      "\n",
      "Epoch 00015: loss improved from 0.04174 to 0.04097, saving model to best_model.SB\n",
      "Epoch 16/200\n",
      "8/8 [==============================] - 0s 407us/step - loss: 0.0401\n",
      "\n",
      "Epoch 00016: loss improved from 0.04097 to 0.04014, saving model to best_model.SB\n",
      "Epoch 17/200\n",
      "8/8 [==============================] - 0s 806us/step - loss: 0.0419\n",
      "\n",
      "Epoch 00017: loss did not improve from 0.04014\n",
      "Epoch 18/200\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0714\n",
      "\n",
      "Epoch 00018: loss did not improve from 0.04014\n",
      "Epoch 19/200\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.0612\n",
      "\n",
      "Epoch 00019: loss did not improve from 0.04014\n",
      "Epoch 20/200\n",
      "8/8 [==============================] - 0s 560us/step - loss: 0.0410\n",
      "\n",
      "Epoch 00020: loss did not improve from 0.04014\n",
      "Epoch 21/200\n",
      "8/8 [==============================] - 0s 907us/step - loss: 0.0391\n",
      "\n",
      "Epoch 00021: loss improved from 0.04014 to 0.03914, saving model to best_model.SB\n",
      "Epoch 22/200\n",
      "8/8 [==============================] - 0s 466us/step - loss: 0.0397\n",
      "\n",
      "Epoch 00022: loss did not improve from 0.03914\n",
      "Epoch 23/200\n",
      "8/8 [==============================] - 0s 432us/step - loss: 0.0392\n",
      "\n",
      "Epoch 00023: loss did not improve from 0.03914\n",
      "Epoch 24/200\n",
      "8/8 [==============================] - 0s 867us/step - loss: 0.0386\n",
      "\n",
      "Epoch 00024: loss improved from 0.03914 to 0.03859, saving model to best_model.SB\n",
      "Epoch 25/200\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.0380\n",
      "\n",
      "Epoch 00025: loss improved from 0.03859 to 0.03796, saving model to best_model.SB\n",
      "Epoch 26/200\n",
      "8/8 [==============================] - 0s 498us/step - loss: 0.0385\n",
      "\n",
      "Epoch 00026: loss did not improve from 0.03796\n",
      "Epoch 27/200\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.0384\n",
      "\n",
      "Epoch 00027: loss did not improve from 0.03796\n",
      "Epoch 28/200\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0376\n",
      "\n",
      "Epoch 00028: loss improved from 0.03796 to 0.03765, saving model to best_model.SB\n",
      "Epoch 29/200\n",
      "8/8 [==============================] - 0s 723us/step - loss: 0.0369\n",
      "\n",
      "Epoch 00029: loss improved from 0.03765 to 0.03687, saving model to best_model.SB\n",
      "Epoch 30/200\n",
      "8/8 [==============================] - 0s 468us/step - loss: 0.0364\n",
      "\n",
      "Epoch 00030: loss improved from 0.03687 to 0.03644, saving model to best_model.SB\n",
      "Epoch 31/200\n",
      "8/8 [==============================] - 0s 506us/step - loss: 0.0381\n",
      "\n",
      "Epoch 00031: loss did not improve from 0.03644\n",
      "Epoch 32/200\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0417\n",
      "\n",
      "Epoch 00032: loss did not improve from 0.03644\n",
      "Epoch 33/200\n",
      "8/8 [==============================] - 0s 755us/step - loss: 0.0526\n",
      "\n",
      "Epoch 00033: loss did not improve from 0.03644\n",
      "Epoch 34/200\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0682\n",
      "\n",
      "Epoch 00034: loss did not improve from 0.03644\n",
      "Epoch 35/200\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.0356\n",
      "\n",
      "Epoch 00035: loss improved from 0.03644 to 0.03564, saving model to best_model.SB\n",
      "Epoch 36/200\n",
      "8/8 [==============================] - 0s 391us/step - loss: 0.0351\n",
      "\n",
      "Epoch 00036: loss improved from 0.03564 to 0.03510, saving model to best_model.SB\n",
      "Epoch 37/200\n",
      "8/8 [==============================] - 0s 273us/step - loss: 0.0345\n",
      "\n",
      "Epoch 00037: loss improved from 0.03510 to 0.03453, saving model to best_model.SB\n",
      "Epoch 38/200\n",
      "8/8 [==============================] - 0s 722us/step - loss: 0.0343\n",
      "\n",
      "Epoch 00038: loss improved from 0.03453 to 0.03435, saving model to best_model.SB\n",
      "Epoch 39/200\n",
      "8/8 [==============================] - 0s 767us/step - loss: 0.0349\n",
      "\n",
      "Epoch 00039: loss did not improve from 0.03435\n",
      "Epoch 40/200\n",
      "8/8 [==============================] - 0s 608us/step - loss: 0.0342\n",
      "\n",
      "Epoch 00040: loss improved from 0.03435 to 0.03419, saving model to best_model.SB\n",
      "Epoch 41/200\n",
      "8/8 [==============================] - 0s 821us/step - loss: 0.0335\n",
      "\n",
      "Epoch 00041: loss improved from 0.03419 to 0.03348, saving model to best_model.SB\n",
      "Epoch 42/200\n",
      "8/8 [==============================] - 0s 320us/step - loss: 0.0327\n",
      "\n",
      "Epoch 00042: loss improved from 0.03348 to 0.03272, saving model to best_model.SB\n",
      "Epoch 43/200\n",
      "8/8 [==============================] - 0s 981us/step - loss: 0.0340\n",
      "\n",
      "Epoch 00043: loss did not improve from 0.03272\n",
      "Epoch 44/200\n",
      "8/8 [==============================] - 0s 423us/step - loss: 0.0540\n",
      "\n",
      "Epoch 00044: loss did not improve from 0.03272\n",
      "Epoch 45/200\n",
      "8/8 [==============================] - 0s 372us/step - loss: 0.0607\n",
      "\n",
      "Epoch 00045: loss did not improve from 0.03272\n",
      "Epoch 46/200\n",
      "8/8 [==============================] - 0s 645us/step - loss: 0.0329\n",
      "\n",
      "Epoch 00046: loss did not improve from 0.03272\n",
      "Epoch 47/200\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.0323\n",
      "\n",
      "Epoch 00047: loss improved from 0.03272 to 0.03233, saving model to best_model.SB\n",
      "Epoch 48/200\n",
      "8/8 [==============================] - 0s 321us/step - loss: 0.0317\n",
      "\n",
      "Epoch 00048: loss improved from 0.03233 to 0.03174, saving model to best_model.SB\n",
      "Epoch 49/200\n",
      "8/8 [==============================] - 0s 764us/step - loss: 0.0311\n",
      "\n",
      "Epoch 00049: loss improved from 0.03174 to 0.03112, saving model to best_model.SB\n",
      "Epoch 50/200\n",
      "8/8 [==============================] - 0s 918us/step - loss: 0.0310\n",
      "\n",
      "Epoch 00050: loss improved from 0.03112 to 0.03096, saving model to best_model.SB\n",
      "Epoch 51/200\n",
      "8/8 [==============================] - 0s 872us/step - loss: 0.0314\n",
      "\n",
      "Epoch 00051: loss did not improve from 0.03096\n",
      "Epoch 52/200\n",
      "8/8 [==============================] - 0s 406us/step - loss: 0.0307\n",
      "\n",
      "Epoch 00052: loss improved from 0.03096 to 0.03070, saving model to best_model.SB\n",
      "Epoch 53/200\n",
      "8/8 [==============================] - 0s 535us/step - loss: 0.0299\n",
      "\n",
      "Epoch 00053: loss improved from 0.03070 to 0.02992, saving model to best_model.SB\n",
      "Epoch 54/200\n",
      "8/8 [==============================] - 0s 703us/step - loss: 0.0292\n",
      "\n",
      "Epoch 00054: loss improved from 0.02992 to 0.02915, saving model to best_model.SB\n",
      "Epoch 55/200\n",
      "8/8 [==============================] - 0s 394us/step - loss: 0.0306\n",
      "\n",
      "Epoch 00055: loss did not improve from 0.02915\n",
      "Epoch 56/200\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.0347\n",
      "\n",
      "Epoch 00056: loss did not improve from 0.02915\n",
      "Epoch 57/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8/8 [==============================] - 0s 965us/step - loss: 0.0694\n",
      "\n",
      "Epoch 00057: loss did not improve from 0.02915\n",
      "Epoch 58/200\n",
      "8/8 [==============================] - 0s 666us/step - loss: 0.0404\n",
      "\n",
      "Epoch 00058: loss did not improve from 0.02915\n",
      "Epoch 59/200\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.0375\n",
      "\n",
      "Epoch 00059: loss did not improve from 0.02915\n",
      "Epoch 60/200\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.0407\n",
      "\n",
      "Epoch 00060: loss did not improve from 0.02915\n",
      "Epoch 61/200\n",
      "8/8 [==============================] - 0s 438us/step - loss: 0.0318\n",
      "\n",
      "Epoch 00061: loss did not improve from 0.02915\n",
      "Epoch 62/200\n",
      "8/8 [==============================] - 0s 650us/step - loss: 0.0303\n",
      "\n",
      "Epoch 00062: loss did not improve from 0.02915\n",
      "Epoch 63/200\n",
      "8/8 [==============================] - 0s 788us/step - loss: 0.0290\n",
      "\n",
      "Epoch 00063: loss improved from 0.02915 to 0.02896, saving model to best_model.SB\n",
      "Epoch 64/200\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.0270\n",
      "\n",
      "Epoch 00064: loss improved from 0.02896 to 0.02703, saving model to best_model.SB\n",
      "Epoch 65/200\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.0274\n",
      "\n",
      "Epoch 00065: loss did not improve from 0.02703\n",
      "Epoch 66/200\n",
      "8/8 [==============================] - 0s 692us/step - loss: 0.0272\n",
      "\n",
      "Epoch 00066: loss did not improve from 0.02703\n",
      "Epoch 67/200\n",
      "8/8 [==============================] - 0s 634us/step - loss: 0.0266\n",
      "\n",
      "Epoch 00067: loss improved from 0.02703 to 0.02663, saving model to best_model.SB\n",
      "Epoch 68/200\n",
      "8/8 [==============================] - 0s 671us/step - loss: 0.0260\n",
      "\n",
      "Epoch 00068: loss improved from 0.02663 to 0.02598, saving model to best_model.SB\n",
      "Epoch 69/200\n",
      "8/8 [==============================] - 0s 360us/step - loss: 0.0259\n",
      "\n",
      "Epoch 00069: loss improved from 0.02598 to 0.02595, saving model to best_model.SB\n",
      "Epoch 70/200\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.0262\n",
      "\n",
      "Epoch 00070: loss did not improve from 0.02595\n",
      "Epoch 71/200\n",
      "8/8 [==============================] - 0s 928us/step - loss: 0.0255\n",
      "\n",
      "Epoch 00071: loss improved from 0.02595 to 0.02547, saving model to best_model.SB\n",
      "Epoch 72/200\n",
      "8/8 [==============================] - 0s 391us/step - loss: 0.0247\n",
      "\n",
      "Epoch 00072: loss improved from 0.02547 to 0.02465, saving model to best_model.SB\n",
      "Epoch 73/200\n",
      "8/8 [==============================] - 0s 494us/step - loss: 0.0242\n",
      "\n",
      "Epoch 00073: loss improved from 0.02465 to 0.02419, saving model to best_model.SB\n",
      "Epoch 74/200\n",
      "8/8 [==============================] - 0s 417us/step - loss: 0.0249\n",
      "\n",
      "Epoch 00074: loss did not improve from 0.02419\n",
      "Epoch 75/200\n",
      "8/8 [==============================] - 0s 528us/step - loss: 0.0240\n",
      "\n",
      "Epoch 00075: loss improved from 0.02419 to 0.02400, saving model to best_model.SB\n",
      "Epoch 76/200\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0230\n",
      "\n",
      "Epoch 00076: loss improved from 0.02400 to 0.02300, saving model to best_model.SB\n",
      "Epoch 77/200\n",
      "8/8 [==============================] - 0s 657us/step - loss: 0.0220\n",
      "\n",
      "Epoch 00077: loss improved from 0.02300 to 0.02204, saving model to best_model.SB\n",
      "Epoch 78/200\n",
      "8/8 [==============================] - 0s 392us/step - loss: 0.0249\n",
      "\n",
      "Epoch 00078: loss did not improve from 0.02204\n",
      "Epoch 79/200\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.0347\n",
      "\n",
      "Epoch 00079: loss did not improve from 0.02204\n",
      "Epoch 80/200\n",
      "8/8 [==============================] - 0s 494us/step - loss: 0.0895\n",
      "\n",
      "Epoch 00080: loss did not improve from 0.02204\n",
      "Epoch 81/200\n",
      "8/8 [==============================] - 0s 371us/step - loss: 0.0216\n",
      "\n",
      "Epoch 00081: loss improved from 0.02204 to 0.02159, saving model to best_model.SB\n",
      "Epoch 82/200\n",
      "8/8 [==============================] - 0s 620us/step - loss: 0.0210\n",
      "\n",
      "Epoch 00082: loss improved from 0.02159 to 0.02097, saving model to best_model.SB\n",
      "Epoch 83/200\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.0213\n",
      "\n",
      "Epoch 00083: loss did not improve from 0.02097\n",
      "Epoch 84/200\n",
      "8/8 [==============================] - 0s 618us/step - loss: 0.0328\n",
      "\n",
      "Epoch 00084: loss did not improve from 0.02097\n",
      "Epoch 85/200\n",
      "8/8 [==============================] - 0s 979us/step - loss: 0.0635\n",
      "\n",
      "Epoch 00085: loss did not improve from 0.02097\n",
      "Epoch 86/200\n",
      "8/8 [==============================] - 0s 767us/step - loss: 0.0273\n",
      "\n",
      "Epoch 00086: loss did not improve from 0.02097\n",
      "Epoch 87/200\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.0362\n",
      "\n",
      "Epoch 00087: loss did not improve from 0.02097\n",
      "Epoch 88/200\n",
      "8/8 [==============================] - 0s 460us/step - loss: 0.0416\n",
      "\n",
      "Epoch 00088: loss did not improve from 0.02097\n",
      "Epoch 89/200\n",
      "8/8 [==============================] - 0s 716us/step - loss: 0.0284\n",
      "\n",
      "Epoch 00089: loss did not improve from 0.02097\n",
      "Epoch 90/200\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0410\n",
      "\n",
      "Epoch 00090: loss did not improve from 0.02097\n",
      "Epoch 91/200\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.0238\n",
      "\n",
      "Epoch 00091: loss did not improve from 0.02097\n",
      "Epoch 92/200\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.0246\n",
      "\n",
      "Epoch 00092: loss did not improve from 0.02097\n",
      "Epoch 93/200\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0201\n",
      "\n",
      "Epoch 00093: loss improved from 0.02097 to 0.02012, saving model to best_model.SB\n",
      "Epoch 94/200\n",
      "8/8 [==============================] - 0s 779us/step - loss: 0.0205\n",
      "\n",
      "Epoch 00094: loss did not improve from 0.02012\n",
      "Epoch 95/200\n",
      "8/8 [==============================] - 0s 381us/step - loss: 0.0221\n",
      "\n",
      "Epoch 00095: loss did not improve from 0.02012\n",
      "Epoch 96/200\n",
      "8/8 [==============================] - 0s 369us/step - loss: 0.0199\n",
      "\n",
      "Epoch 00096: loss improved from 0.02012 to 0.01993, saving model to best_model.SB\n",
      "Epoch 97/200\n",
      "8/8 [==============================] - 0s 445us/step - loss: 0.0195\n",
      "\n",
      "Epoch 00097: loss improved from 0.01993 to 0.01946, saving model to best_model.SB\n",
      "Epoch 98/200\n",
      "8/8 [==============================] - 0s 515us/step - loss: 0.0192\n",
      "\n",
      "Epoch 00098: loss improved from 0.01946 to 0.01923, saving model to best_model.SB\n",
      "Epoch 99/200\n",
      "8/8 [==============================] - 0s 605us/step - loss: 0.0198\n",
      "\n",
      "Epoch 00099: loss did not improve from 0.01923\n",
      "Epoch 100/200\n",
      "8/8 [==============================] - 0s 598us/step - loss: 0.0192\n",
      "\n",
      "Epoch 00100: loss did not improve from 0.01923\n",
      "Epoch 101/200\n",
      "8/8 [==============================] - 0s 791us/step - loss: 0.0186\n",
      "\n",
      "Epoch 00101: loss improved from 0.01923 to 0.01865, saving model to best_model.SB\n",
      "Epoch 102/200\n",
      "8/8 [==============================] - 0s 939us/step - loss: 0.0181\n",
      "\n",
      "Epoch 00102: loss improved from 0.01865 to 0.01809, saving model to best_model.SB\n",
      "Epoch 103/200\n",
      "8/8 [==============================] - 0s 815us/step - loss: 0.0190\n",
      "\n",
      "Epoch 00103: loss did not improve from 0.01809\n",
      "Epoch 104/200\n",
      "8/8 [==============================] - 0s 790us/step - loss: 0.0184\n",
      "\n",
      "Epoch 00104: loss did not improve from 0.01809\n",
      "Epoch 105/200\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.0176\n",
      "\n",
      "Epoch 00105: loss improved from 0.01809 to 0.01764, saving model to best_model.SB\n",
      "Epoch 106/200\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.0177\n",
      "\n",
      "Epoch 00106: loss did not improve from 0.01764\n",
      "Epoch 107/200\n",
      "8/8 [==============================] - 0s 960us/step - loss: 0.0245\n",
      "\n",
      "Epoch 00107: loss did not improve from 0.01764\n",
      "Epoch 108/200\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.0671\n",
      "\n",
      "Epoch 00108: loss did not improve from 0.01764\n",
      "Epoch 109/200\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0353\n",
      "\n",
      "Epoch 00109: loss did not improve from 0.01764\n",
      "Epoch 110/200\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.0504\n",
      "\n",
      "Epoch 00110: loss did not improve from 0.01764\n",
      "Epoch 111/200\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0318\n",
      "\n",
      "Epoch 00111: loss did not improve from 0.01764\n",
      "Epoch 112/200\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0415\n",
      "\n",
      "Epoch 00112: loss did not improve from 0.01764\n",
      "Epoch 113/200\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.0308\n",
      "\n",
      "Epoch 00113: loss did not improve from 0.01764\n",
      "Epoch 114/200\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0356\n",
      "\n",
      "Epoch 00114: loss did not improve from 0.01764\n",
      "Epoch 115/200\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.0309\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00115: loss did not improve from 0.01764\n",
      "Epoch 116/200\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0311\n",
      "\n",
      "Epoch 00116: loss did not improve from 0.01764\n",
      "Epoch 117/200\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.0315\n",
      "\n",
      "Epoch 00117: loss did not improve from 0.01764\n",
      "Epoch 118/200\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.0276\n",
      "\n",
      "Epoch 00118: loss did not improve from 0.01764\n",
      "Epoch 119/200\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0323\n",
      "\n",
      "Epoch 00119: loss did not improve from 0.01764\n",
      "Epoch 120/200\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0247\n",
      "\n",
      "Epoch 00120: loss did not improve from 0.01764\n",
      "Epoch 121/200\n",
      "8/8 [==============================] - 0s 666us/step - loss: 0.0227\n",
      "\n",
      "Epoch 00121: loss did not improve from 0.01764\n",
      "Epoch 122/200\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0242\n",
      "\n",
      "Epoch 00122: loss did not improve from 0.01764\n",
      "Epoch 123/200\n",
      "8/8 [==============================] - 0s 439us/step - loss: 0.0239\n",
      "\n",
      "Epoch 00123: loss did not improve from 0.01764\n",
      "Epoch 124/200\n",
      "8/8 [==============================] - 0s 350us/step - loss: 0.0236\n",
      "\n",
      "Epoch 00124: loss did not improve from 0.01764\n",
      "Epoch 125/200\n",
      "8/8 [==============================] - 0s 688us/step - loss: 0.0251\n",
      "Restoring model weights from the end of the best epoch\n",
      "\n",
      "Epoch 00125: loss did not improve from 0.01764\n",
      "Epoch 00125: early stopping\n",
      "best epoch =  105\n",
      "smallest loss = 0.017639674246311188\n"
     ]
    }
   ],
   "source": [
    "sgd = keras.optimizers.RMSprop(learning_rate=0.01)\n",
    "model.compile(loss='mean_absolute_error',optimizer=sgd)\n",
    "\n",
    "historyData = model.fit(xarray,df.y3,epochs=200,callbacks=[es,mc])\n",
    "\n",
    "loss_hist = historyData.history['loss']\n",
    "#The above line will return a dictionary, access it's info like this:\n",
    "best_epoch = np.argmin(historyData.history['loss']) + 1\n",
    "print ('best epoch = ', best_epoch)\n",
    "print('smallest loss =', np.min(loss_hist))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "8/8 [==============================] - 1s 85ms/step - loss: 0.0444\n",
      "Epoch 2/200\n",
      "8/8 [==============================] - 0s 426us/step - loss: 0.0420\n",
      "Epoch 3/200\n",
      "8/8 [==============================] - 0s 424us/step - loss: 0.0419\n",
      "Epoch 4/200\n",
      "8/8 [==============================] - 0s 596us/step - loss: 0.0417\n",
      "Epoch 5/200\n",
      "8/8 [==============================] - 0s 546us/step - loss: 0.0416\n",
      "Epoch 6/200\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.0415\n",
      "Epoch 7/200\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0413\n",
      "Epoch 8/200\n",
      "8/8 [==============================] - 0s 595us/step - loss: 0.0412\n",
      "Epoch 9/200\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.0410\n",
      "Epoch 10/200\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.0409\n",
      "Epoch 11/200\n",
      "8/8 [==============================] - 0s 923us/step - loss: 0.0407\n",
      "Epoch 12/200\n",
      "8/8 [==============================] - 0s 999us/step - loss: 0.0406\n",
      "Epoch 13/200\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.0404\n",
      "Epoch 14/200\n",
      "8/8 [==============================] - 0s 965us/step - loss: 0.0402\n",
      "Epoch 15/200\n",
      "8/8 [==============================] - 0s 448us/step - loss: 0.0400\n",
      "Epoch 16/200\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.0399\n",
      "Epoch 17/200\n",
      "8/8 [==============================] - 0s 853us/step - loss: 0.0398\n",
      "Epoch 18/200\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0397\n",
      "Epoch 19/200\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.0395\n",
      "Epoch 20/200\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.0394\n",
      "Epoch 21/200\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.0392\n",
      "Epoch 22/200\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.0390\n",
      "Epoch 23/200\n",
      "8/8 [==============================] - 0s 672us/step - loss: 0.0388\n",
      "Epoch 24/200\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.0387\n",
      "Epoch 25/200\n",
      "8/8 [==============================] - 0s 661us/step - loss: 0.0385\n",
      "Epoch 26/200\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.0383\n",
      "Epoch 27/200\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.0381\n",
      "Epoch 28/200\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.0379\n",
      "Epoch 29/200\n",
      "8/8 [==============================] - 0s 717us/step - loss: 0.0377\n",
      "Epoch 30/200\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.0374\n",
      "Epoch 31/200\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.0375\n",
      "Epoch 32/200\n",
      "8/8 [==============================] - 0s 712us/step - loss: 0.0373\n",
      "Epoch 33/200\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.0371\n",
      "Epoch 34/200\n",
      "8/8 [==============================] - 0s 879us/step - loss: 0.0369\n",
      "Epoch 35/200\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.0367\n",
      "Epoch 36/200\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0365\n",
      "Epoch 37/200\n",
      "8/8 [==============================] - 0s 525us/step - loss: 0.0363\n",
      "Epoch 38/200\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0361\n",
      "Epoch 39/200\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0359\n",
      "Epoch 40/200\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.0357\n",
      "Epoch 41/200\n",
      "8/8 [==============================] - 0s 567us/step - loss: 0.0355\n",
      "Epoch 42/200\n",
      "8/8 [==============================] - 0s 579us/step - loss: 0.0353\n",
      "Epoch 43/200\n",
      "8/8 [==============================] - 0s 588us/step - loss: 0.0350\n",
      "Epoch 44/200\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0350\n",
      "Epoch 45/200\n",
      "8/8 [==============================] - 0s 679us/step - loss: 0.0349\n",
      "Epoch 46/200\n",
      "8/8 [==============================] - 0s 430us/step - loss: 0.0347\n",
      "Epoch 47/200\n",
      "8/8 [==============================] - 0s 555us/step - loss: 0.0345\n",
      "Epoch 48/200\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.0343\n",
      "Epoch 49/200\n",
      "8/8 [==============================] - 0s 368us/step - loss: 0.0341\n",
      "Epoch 50/200\n",
      "8/8 [==============================] - 0s 363us/step - loss: 0.0339\n",
      "Epoch 51/200\n",
      "8/8 [==============================] - 0s 392us/step - loss: 0.0337\n",
      "Epoch 52/200\n",
      "8/8 [==============================] - 0s 361us/step - loss: 0.0334\n",
      "Epoch 53/200\n",
      "8/8 [==============================] - 0s 320us/step - loss: 0.0332\n",
      "Epoch 54/200\n",
      "8/8 [==============================] - 0s 424us/step - loss: 0.0330\n",
      "Epoch 55/200\n",
      "8/8 [==============================] - 0s 320us/step - loss: 0.0327\n",
      "Epoch 56/200\n",
      "8/8 [==============================] - 0s 390us/step - loss: 0.0326\n",
      "Epoch 57/200\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.0326\n",
      "Epoch 58/200\n",
      "8/8 [==============================] - 0s 356us/step - loss: 0.0324\n",
      "Epoch 59/200\n",
      "8/8 [==============================] - 0s 373us/step - loss: 0.0322\n",
      "Epoch 60/200\n",
      "8/8 [==============================] - 0s 557us/step - loss: 0.0320\n",
      "Epoch 61/200\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.0318\n",
      "Epoch 62/200\n",
      "8/8 [==============================] - 0s 360us/step - loss: 0.0315\n",
      "Epoch 63/200\n",
      "8/8 [==============================] - 0s 902us/step - loss: 0.0313\n",
      "Epoch 64/200\n",
      "8/8 [==============================] - 0s 373us/step - loss: 0.0311\n",
      "Epoch 65/200\n",
      "8/8 [==============================] - 0s 378us/step - loss: 0.0308\n",
      "Epoch 66/200\n",
      "8/8 [==============================] - 0s 312us/step - loss: 0.0306\n",
      "Epoch 67/200\n",
      "8/8 [==============================] - 0s 326us/step - loss: 0.0304\n",
      "Epoch 68/200\n",
      "8/8 [==============================] - 0s 421us/step - loss: 0.0303\n",
      "Epoch 69/200\n",
      "8/8 [==============================] - 0s 761us/step - loss: 0.0302\n",
      "Epoch 70/200\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.0300\n",
      "Epoch 71/200\n",
      "8/8 [==============================] - 0s 679us/step - loss: 0.0298\n",
      "Epoch 72/200\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0296\n",
      "Epoch 73/200\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0293\n",
      "Epoch 74/200\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.0291\n",
      "Epoch 75/200\n",
      "8/8 [==============================] - 0s 962us/step - loss: 0.0289\n",
      "Epoch 76/200\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0286\n",
      "Epoch 77/200\n",
      "8/8 [==============================] - 0s 601us/step - loss: 0.0284\n",
      "Epoch 78/200\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0281\n",
      "Epoch 79/200\n",
      "8/8 [==============================] - 0s 674us/step - loss: 0.0280\n",
      "Epoch 80/200\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0280\n",
      "Epoch 81/200\n",
      "8/8 [==============================] - 0s 906us/step - loss: 0.0278\n",
      "Epoch 82/200\n",
      "8/8 [==============================] - 0s 780us/step - loss: 0.0275\n",
      "Epoch 83/200\n",
      "8/8 [==============================] - 0s 408us/step - loss: 0.0273\n",
      "Epoch 84/200\n",
      "8/8 [==============================] - 0s 365us/step - loss: 0.0271\n",
      "Epoch 85/200\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0268\n",
      "Epoch 86/200\n",
      "8/8 [==============================] - 0s 409us/step - loss: 0.0266\n",
      "Epoch 87/200\n",
      "8/8 [==============================] - 0s 367us/step - loss: 0.0263\n",
      "Epoch 88/200\n",
      "8/8 [==============================] - 0s 627us/step - loss: 0.0261\n",
      "Epoch 89/200\n",
      "8/8 [==============================] - 0s 546us/step - loss: 0.0258\n",
      "Epoch 90/200\n",
      "8/8 [==============================] - 0s 384us/step - loss: 0.0259\n",
      "Epoch 91/200\n",
      "8/8 [==============================] - 0s 377us/step - loss: 0.0257\n",
      "Epoch 92/200\n",
      "8/8 [==============================] - 0s 443us/step - loss: 0.0254\n",
      "Epoch 93/200\n",
      "8/8 [==============================] - 0s 470us/step - loss: 0.0252\n",
      "Epoch 94/200\n",
      "8/8 [==============================] - 0s 826us/step - loss: 0.0250\n",
      "Epoch 95/200\n",
      "8/8 [==============================] - 0s 654us/step - loss: 0.0247\n",
      "Epoch 96/200\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.0245\n",
      "Epoch 97/200\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0242\n",
      "Epoch 98/200\n",
      "8/8 [==============================] - 0s 827us/step - loss: 0.0240\n",
      "Epoch 99/200\n",
      "8/8 [==============================] - 0s 688us/step - loss: 0.0237\n",
      "Epoch 100/200\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.0237\n",
      "Epoch 101/200\n",
      "8/8 [==============================] - 0s 572us/step - loss: 0.0235\n",
      "Epoch 102/200\n",
      "8/8 [==============================] - 0s 696us/step - loss: 0.0233\n",
      "Epoch 103/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8/8 [==============================] - 0s 687us/step - loss: 0.0231\n",
      "Epoch 104/200\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0228\n",
      "Epoch 105/200\n",
      "8/8 [==============================] - 0s 385us/step - loss: 0.0226\n",
      "Epoch 106/200\n",
      "8/8 [==============================] - 0s 474us/step - loss: 0.0223\n",
      "Epoch 107/200\n",
      "8/8 [==============================] - 0s 373us/step - loss: 0.0221\n",
      "Epoch 108/200\n",
      "8/8 [==============================] - 0s 336us/step - loss: 0.0218\n",
      "Epoch 109/200\n",
      "8/8 [==============================] - 0s 389us/step - loss: 0.0215\n",
      "Epoch 110/200\n",
      "8/8 [==============================] - 0s 293us/step - loss: 0.0215\n",
      "Epoch 111/200\n",
      "8/8 [==============================] - 0s 405us/step - loss: 0.0214\n",
      "Epoch 112/200\n",
      "8/8 [==============================] - 0s 442us/step - loss: 0.0211\n",
      "Epoch 113/200\n",
      "8/8 [==============================] - 0s 327us/step - loss: 0.0209\n",
      "Epoch 114/200\n",
      "8/8 [==============================] - 0s 611us/step - loss: 0.0206\n",
      "Epoch 115/200\n",
      "8/8 [==============================] - 0s 595us/step - loss: 0.0204\n",
      "Epoch 116/200\n",
      "8/8 [==============================] - 0s 866us/step - loss: 0.0201\n",
      "Epoch 117/200\n",
      "8/8 [==============================] - 0s 757us/step - loss: 0.0198\n",
      "Epoch 118/200\n",
      "8/8 [==============================] - 0s 422us/step - loss: 0.0195\n",
      "Epoch 119/200\n",
      "8/8 [==============================] - 0s 302us/step - loss: 0.0193\n",
      "Epoch 120/200\n",
      "8/8 [==============================] - 0s 327us/step - loss: 0.0194\n",
      "Epoch 121/200\n",
      "8/8 [==============================] - 0s 911us/step - loss: 0.0191\n",
      "Epoch 122/200\n",
      "8/8 [==============================] - 0s 742us/step - loss: 0.0189\n",
      "Epoch 123/200\n",
      "8/8 [==============================] - 0s 893us/step - loss: 0.0189\n",
      "Epoch 124/200\n",
      "8/8 [==============================] - 0s 471us/step - loss: 0.0187\n",
      "Epoch 125/200\n",
      "8/8 [==============================] - 0s 789us/step - loss: 0.0185\n",
      "Epoch 126/200\n",
      "8/8 [==============================] - 0s 457us/step - loss: 0.0183\n",
      "Epoch 127/200\n",
      "8/8 [==============================] - 0s 557us/step - loss: 0.0180\n",
      "Epoch 128/200\n",
      "8/8 [==============================] - 0s 687us/step - loss: 0.0178\n",
      "Epoch 129/200\n",
      "8/8 [==============================] - 0s 481us/step - loss: 0.0176\n",
      "Epoch 130/200\n",
      "8/8 [==============================] - 0s 638us/step - loss: 0.0173\n",
      "Epoch 131/200\n",
      "8/8 [==============================] - 0s 760us/step - loss: 0.0171\n",
      "Epoch 132/200\n",
      "8/8 [==============================] - 0s 739us/step - loss: 0.0171\n",
      "Epoch 133/200\n",
      "8/8 [==============================] - 0s 725us/step - loss: 0.0177\n",
      "Epoch 134/200\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.0167\n",
      "Epoch 135/200\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.0169\n",
      "Epoch 136/200\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0166\n",
      "Epoch 137/200\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0164\n",
      "Epoch 138/200\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.0165\n",
      "Epoch 139/200\n",
      "8/8 [==============================] - 0s 525us/step - loss: 0.0161\n",
      "Epoch 140/200\n",
      "8/8 [==============================] - 0s 303us/step - loss: 0.0159\n",
      "Epoch 141/200\n",
      "8/8 [==============================] - 0s 546us/step - loss: 0.0158\n",
      "Epoch 142/200\n",
      "8/8 [==============================] - 0s 641us/step - loss: 0.0156\n",
      "Epoch 143/200\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.0154\n",
      "Epoch 144/200\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.0152\n",
      "Epoch 145/200\n",
      "8/8 [==============================] - 0s 706us/step - loss: 0.0150\n",
      "Epoch 146/200\n",
      "8/8 [==============================] - 0s 485us/step - loss: 0.0147\n",
      "Epoch 147/200\n",
      "8/8 [==============================] - 0s 712us/step - loss: 0.0148\n",
      "Epoch 148/200\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.0162\n",
      "Epoch 149/200\n",
      "8/8 [==============================] - 0s 431us/step - loss: 0.0159\n",
      "Epoch 150/200\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.0144\n",
      "Epoch 151/200\n",
      "8/8 [==============================] - 0s 457us/step - loss: 0.0143\n",
      "Epoch 152/200\n",
      "8/8 [==============================] - 0s 541us/step - loss: 0.0142\n",
      "Epoch 153/200\n",
      "8/8 [==============================] - 0s 311us/step - loss: 0.0157\n",
      "Epoch 154/200\n",
      "8/8 [==============================] - 0s 885us/step - loss: 0.0154\n",
      "Epoch 155/200\n",
      "8/8 [==============================] - 0s 920us/step - loss: 0.0140\n",
      "Epoch 156/200\n",
      "8/8 [==============================] - 0s 783us/step - loss: 0.0139\n",
      "Epoch 157/200\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.0155\n",
      "Epoch 158/200\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0151\n",
      "Epoch 159/200\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.0138\n",
      "Epoch 160/200\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0138\n",
      "Epoch 161/200\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.0154\n",
      "Epoch 162/200\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0150\n",
      "Epoch 163/200\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0136\n",
      "Epoch 164/200\n",
      "8/8 [==============================] - 0s 692us/step - loss: 0.0153\n",
      "Epoch 165/200\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.0148\n",
      "Epoch 166/200\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0135\n",
      "Epoch 167/200\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.0153\n",
      "Epoch 168/200\n",
      "8/8 [==============================] - 0s 816us/step - loss: 0.0146\n",
      "Epoch 169/200\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0134\n",
      "Epoch 170/200\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0153\n",
      "Epoch 171/200\n",
      "8/8 [==============================] - 0s 739us/step - loss: 0.0145\n",
      "Epoch 172/200\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.0133\n",
      "Epoch 173/200\n",
      "8/8 [==============================] - 0s 533us/step - loss: 0.0133\n",
      "Epoch 174/200\n",
      "8/8 [==============================] - 0s 412us/step - loss: 0.0133\n",
      "Epoch 175/200\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.0133\n",
      "Epoch 176/200\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.0157\n",
      "Epoch 177/200\n",
      "8/8 [==============================] - 0s 595us/step - loss: 0.0145\n",
      "Epoch 178/200\n",
      "8/8 [==============================] - 0s 972us/step - loss: 0.0134\n",
      "Epoch 179/200\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0145\n",
      "Epoch 180/200\n",
      "8/8 [==============================] - 0s 856us/step - loss: 0.0134\n",
      "Epoch 181/200\n",
      "8/8 [==============================] - 0s 507us/step - loss: 0.0134\n",
      "Epoch 182/200\n",
      "8/8 [==============================] - 0s 540us/step - loss: 0.0146\n",
      "Epoch 183/200\n",
      "8/8 [==============================] - 0s 506us/step - loss: 0.0134\n",
      "Epoch 184/200\n",
      "8/8 [==============================] - 0s 476us/step - loss: 0.0146\n",
      "Epoch 185/200\n",
      "8/8 [==============================] - 0s 568us/step - loss: 0.0134\n",
      "Epoch 186/200\n",
      "8/8 [==============================] - 0s 598us/step - loss: 0.0134\n",
      "Epoch 187/200\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.0147\n",
      "Epoch 188/200\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.0133\n",
      "Epoch 189/200\n",
      "8/8 [==============================] - 0s 666us/step - loss: 0.0147\n",
      "Epoch 190/200\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.0134\n",
      "Epoch 191/200\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.0134\n",
      "Epoch 192/200\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0148\n",
      "Epoch 193/200\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.0133\n",
      "Epoch 194/200\n",
      "8/8 [==============================] - 0s 428us/step - loss: 0.0134\n",
      "Epoch 195/200\n",
      "8/8 [==============================] - 0s 513us/step - loss: 0.0148\n",
      "Restoring model weights from the end of the best epoch\n",
      "Epoch 00195: early stopping\n",
      "best epoch =  175\n",
      "smallest loss = 0.013304144144058228\n"
     ]
    }
   ],
   "source": [
    "sgd = keras.optimizers.RMSprop(learning_rate=0.001)\n",
    "model.compile(loss='mean_absolute_error',optimizer=sgd)\n",
    "\n",
    "historyData = model.fit(xarray,df.y3,epochs=200,callbacks=[es,mc])\n",
    "\n",
    "loss_hist = historyData.history['loss']\n",
    "#The above line will return a dictionary, access it's info like this:\n",
    "best_epoch = np.argmin(historyData.history['loss']) + 1\n",
    "print ('best epoch = ', best_epoch)\n",
    "print('smallest loss =', np.min(loss_hist))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.7236678 ]\n",
      " [-0.1818915 ]\n",
      " [ 0.68304104]]\n",
      "w01 =  -0.7236678 w02 =  -0.1818915 w03 =  0.68304104\n",
      "[0.04664435]\n",
      "b1 =  [0.04664435]\n",
      "[[-1.424301]]\n",
      "w12 =  -1.424301\n",
      "[0.35166764]\n",
      "b2 =  [0.35166764]\n",
      "[[0.85712683]]\n",
      "w23 =  0.85712683\n",
      "[0.4966599]\n",
      "b3 =  [0.4966599]\n",
      "x01/20.2,  x02/14.5,   x03/308.0,  y3/32.4,  a3:\n",
      "0.9900990099009901 0.896551724137931 1.009090909090909 0.9558346964599856 [[0.9614737]]\n",
      "0.9900990099009901 1.0 1.0 0.9968828122588808 [[0.9876081]]\n",
      "0.9900990099009901 1.0551724137931036 0.9935064935064936 0.9721922162896206 [[1.002424]]\n",
      "1.0 0.896551724137931 1.009090909090909 0.9539829017622912 [[0.9690231]]\n",
      "0.9900990099009901 1.0 1.0 1.003055461251196 [[0.9876081]]\n",
      "1.0 1.0551724137931036 0.9935064935064936 0.9691058917934631 [[1.009681]]\n",
      "1.188118811881188 0.896551724137931 1.009090909090909 1.0984228881824636 [[1.1026455]]\n",
      "1.7821782178217822 1.0 1.0 1.4320545662170918 [[1.4375385]]\n",
      "  \n",
      "x01,  x02,   x03,  y3,  a3*32.4:\n",
      "20.0 13.0 310.8 30.969044165303533 [[31.151749]]\n",
      "20.0 14.5 308.0 32.29900311718774 [[31.998503]]\n",
      "20.0 15.3 306.0 31.499027807783705 [[32.47854]]\n",
      "20.2 13.0 310.8 30.909046017098234 [[31.39635]]\n",
      "20.0 14.5 308.0 32.498996944538746 [[31.998503]]\n",
      "20.2 15.3 306.0 31.3990308941082 [[32.713665]]\n",
      "23.999999999999996 13.0 310.8 35.58890157711182 [[35.725716]]\n",
      "36.0 14.5 308.0 46.398567945433776 [[46.57625]]\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "#For results of training network:\n",
    "\n",
    "#keras.layer.get_weights() function retrieves weight values\n",
    "first_layer_weights = model.layers[0].get_weights()[0]\n",
    "w01 = first_layer_weights[0][0]\n",
    "w02 = first_layer_weights[1][0]\n",
    "w03 = first_layer_weights[2][0]\n",
    "first_layer_bias  = model.layers[0].get_weights()[1]\n",
    "b1 = first_layer_bias\n",
    "second_layer_weights = model.layers[1].get_weights()[0]\n",
    "w12 = second_layer_weights[0][0]\n",
    "second_layer_bias  = model.layers[1].get_weights()[1]\n",
    "b2 = second_layer_bias\n",
    "third_layer_weights = model.layers[2].get_weights()[0]\n",
    "w23 = third_layer_weights[0][0]\n",
    "third_layer_bias  = model.layers[2].get_weights()[1]\n",
    "b3 = third_layer_bias\n",
    "\n",
    "#print weights and biases\n",
    "print (first_layer_weights)\n",
    "print ('w01 = ', w01, 'w02 = ', w02, 'w03 = ', w03)\n",
    "print (first_layer_bias)\n",
    "print ('b1 = ', b1)\n",
    "print (second_layer_weights)\n",
    "print ('w12 = ', w12)\n",
    "print (second_layer_bias)\n",
    "print ('b2 = ', b2)\n",
    "print (third_layer_weights)\n",
    "print ('w23 = ', w23)\n",
    "print (third_layer_bias)\n",
    "print ('b3 = ', b3)\n",
    "\n",
    "#use model.predict() function to print model predictions for data conditions\n",
    "xarray= np.array(xdata)\n",
    "print ('x01/20.2,  x02/14.5,   x03/308.0,  y3/32.4,  a3:')\n",
    "test = []\n",
    "for i in range(0,8): \n",
    "    test = [[xarray[i][0], xarray[i][1], xarray[i][2]]]\n",
    "    testarray = np.array(test)\n",
    "    a3 = model.predict(testarray)\n",
    "    print (xarray[i][0], xarray[i][1], xarray[i][2], df.y3[i], a3)\n",
    "print('  ')\n",
    "print ('x01,  x02,   x03,  y3,  a3*32.4:')\n",
    "for i in range(0,8): \n",
    "    test = [[xarray[i][0], xarray[i][1], xarray[i][2]]]\n",
    "    testarray = np.array(test)\n",
    "    a3 = model.predict(testarray)\n",
    "    print (xarray[i][0]*20.2, xarray[i][1]*14.5, xarray[i][2]*308.0, df.y3[i]*32.4, a3*32.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
