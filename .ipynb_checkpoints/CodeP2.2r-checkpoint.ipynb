{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    0.990099\n",
      "1    0.990099\n",
      "2    0.990099\n",
      "3    1.000000\n",
      "4    0.990099\n",
      "5    1.000000\n",
      "6    1.188119\n",
      "7    1.782178\n",
      "Name: x01, dtype: float64 0    0.896552\n",
      "1    1.000000\n",
      "2    1.055172\n",
      "3    0.896552\n",
      "4    1.000000\n",
      "5    1.055172\n",
      "6    0.896552\n",
      "7    1.000000\n",
      "Name: x02, dtype: float64 0    1.009091\n",
      "1    1.000000\n",
      "2    0.993506\n",
      "3    1.009091\n",
      "4    1.000000\n",
      "5    0.993506\n",
      "6    1.009091\n",
      "7    1.000000\n",
      "Name: x03, dtype: float64 0    0.955835\n",
      "1    0.996883\n",
      "2    0.972192\n",
      "3    0.953983\n",
      "4    1.003055\n",
      "5    0.969106\n",
      "6    1.098423\n",
      "7    1.432055\n",
      "Name: y3, dtype: float64\n",
      "[[0.9900990099009901, 0.896551724137931, 1.009090909090909], [0.9900990099009901, 1.0, 1.0], [0.9900990099009901, 1.0551724137931036, 0.9935064935064936], [1.0, 0.896551724137931, 1.009090909090909], [0.9900990099009901, 1.0, 1.0], [1.0, 1.0551724137931036, 0.9935064935064936], [1.188118811881188, 0.896551724137931, 1.009090909090909], [1.7821782178217822, 1.0, 1.0]]\n",
      "[[0.99009901 0.89655172 1.00909091]\n",
      " [0.99009901 1.         1.        ]\n",
      " [0.99009901 1.05517241 0.99350649]\n",
      " [1.         0.89655172 1.00909091]\n",
      " [0.99009901 1.         1.        ]\n",
      " [1.         1.05517241 0.99350649]\n",
      " [1.18811881 0.89655172 1.00909091]\n",
      " [1.78217822 1.         1.        ]]\n"
     ]
    }
   ],
   "source": [
    "'''>>>>> start CodeP2.2\n",
    "    V.P. Carey ME249, Spring 2021\n",
    "\n",
    "Intro to Neural Network Modeling \n",
    "Keras model for comparison with first principles model'''\n",
    "\n",
    "#import useful packages\n",
    "import keras\n",
    "import pandas as pd\n",
    "from keras.models import Sequential\n",
    "import numpy as np\n",
    "import keras.backend as kb\n",
    "import tensorflow as tf\n",
    "\n",
    "#the follwoing 2 lines are only needed for Mac OS machines\n",
    "# import os\n",
    "# os.environ['KMP_DUPLICATE_LIB_OK']='True'\n",
    "\n",
    "#raw data in dictionary form x01, x02, x03, y3\n",
    "my_dict = { \n",
    "    'x01' : [20., 20., 20., 20.2, 20., 20.2, 24.0, 36.],\n",
    "    'x02' : [13., 14.5, 15.3, 13., 14.5, 15.3, 13., 14.5],\n",
    "    'x03' : [310.8, 308.0, 306.0, 310.8, 308.0, 306.0, 310.8, 308.0],\n",
    "    'y3' : [30.97, 32.3, 31.5, 30.91, 32.5, 31.4, 35.59, 46.4]\n",
    "}\n",
    "#normalized inputs in array\n",
    "xdata = []\n",
    "xdata = [[20./20.2, 13.0/14.5, 310.8/308.0], [20./20.2, 14.5/14.5, 308.0/308.0]] \n",
    "xdata.append([20./20.2, 15.3/14.5, 306.0/308.0])\n",
    "xdata.append([20.2/20.2, 13.0/14.5, 310.8/308.0]) \n",
    "xdata.append([20./20.2, 14.5/14.5, 308.0/308.0]) \n",
    "xdata.append([20.2/20.2, 15.3/14.5, 306.0/308.0]) \n",
    "xdata.append([24./20.2, 13.0/14.5, 310.8/308.0]) \n",
    "xdata.append([36./20.2, 14.5/14.5, 308.0/308.0]) \n",
    "#data frame\n",
    "df = pd.DataFrame(my_dict)\n",
    "#devide by the median to normalize \n",
    "df.x01= df.x01/20.2\n",
    "df.x02= df.x02/14.5\n",
    "df.x03= df.x03/308.0\n",
    "#normalize output array\n",
    "df.y3= df.y3/32.401\n",
    "df.head\n",
    "print (df.x01, df.x02, df.x03, df.y3)\n",
    "\n",
    "xarray= np.array(xdata)\n",
    "print (xdata)\n",
    "print (xarray)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define model\n",
    "\n",
    "#As seen below, we have created three dense layers each with just one neuron. \n",
    "#A dense layer is a layer in neural network that’s fully connected. \n",
    "#In other words, all the neurons in one layer are connected to all other neurons in the next layer.\n",
    "#In the first layer, we need to provide the input shape, which is 3 in this case. \n",
    "#The activation function we have chosen is ReLU, which stands for rectified linear unit.\n",
    "\n",
    "from keras import backend as K\n",
    "#initialize weights with values between -0.2 and 1.2\n",
    "initializer = tf.keras.initializers.RandomUniform(minval= -0.2, maxval=1.2)\n",
    "\n",
    "# define three layer model with one neuron in each layer\n",
    "model = keras.Sequential([\n",
    "    keras.layers.Dense(1, activation=K.elu, input_shape=[3]),\n",
    "    keras.layers.Dense(1, activation=K.elu),\n",
    "    keras.layers.Dense(1, activation=K.elu)\n",
    "  ])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We’re using RMSprop as our optimizer here. RMSprop stands for Root Mean Square Propagation. \n",
    "#It’s one of the most popular gradient descent optimization algorithms for deep learning networks. \n",
    "#RMSprop is an optimizer that’s reliable and fast.\n",
    "#We’re compiling the mode using the model.compile function. The loss function used here \n",
    "#is mean absolute error. After the compilation of the model, we’ll use the fit method with 100 epochs.\n",
    "\n",
    "#Running model.fit successive times extends the calculation to addtional epochs.\n",
    "\n",
    "sgd = keras.optimizers.RMSprop(learning_rate=0.1)\n",
    "model.compile(loss='mean_absolute_error',optimizer=sgd)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#After the compilation of the model, we’ll use the fit method with 500 epochs.\n",
    "#I started with epochs value of 100 and then tested the model after training. \n",
    "#The prediction was not that good. Then I modified the number of epochs to 200 and tested the model again. \n",
    "#Accuracy had improved slightly, but figured I’d give it one more try. Finally, at 500 epochs \n",
    "#I found acceptable prediction accuracy.\n",
    "\n",
    "#The fit method takes three parameters; namely, x, y, and number of epochs. \n",
    "#During model training, if all the batches of data are seen by the model once, \n",
    "#we say that one epoch has been completed.\n",
    "\n",
    "# Add an early stopping callback\n",
    "es = keras.callbacks.EarlyStopping(\n",
    "    monitor='loss', \n",
    "    mode='min', \n",
    "    patience = 20, \n",
    "    restore_best_weights = True, \n",
    "    verbose=1)\n",
    "# Add a checkpoint where loss is minimum, and save that model\n",
    "mc = keras.callbacks.ModelCheckpoint('best_model.SB', monitor='loss', \n",
    "                     mode='min',  verbose=1, save_best_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "8/8 [==============================] - 1s 118ms/step - loss: 1.1973\n",
      "Epoch 2/100\n",
      "8/8 [==============================] - 0s 566us/step - loss: 0.7643\n",
      "Epoch 3/100\n",
      "8/8 [==============================] - 0s 475us/step - loss: 0.4853\n",
      "Epoch 4/100\n",
      "8/8 [==============================] - 0s 587us/step - loss: 0.2830\n",
      "Epoch 5/100\n",
      "8/8 [==============================] - 0s 692us/step - loss: 0.1006\n",
      "Epoch 6/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.1650\n",
      "Epoch 7/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.0853\n",
      "Epoch 8/100\n",
      "8/8 [==============================] - 0s 744us/step - loss: 0.0849\n",
      "Epoch 9/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.0846\n",
      "Epoch 10/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.0842\n",
      "Epoch 11/100\n",
      "8/8 [==============================] - 0s 638us/step - loss: 0.0838\n",
      "Epoch 12/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0833\n",
      "Epoch 13/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.0830\n",
      "Epoch 14/100\n",
      "8/8 [==============================] - 0s 683us/step - loss: 0.0988\n",
      "Epoch 15/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1403\n",
      "Epoch 16/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.1461\n",
      "Epoch 17/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.0968\n",
      "Epoch 18/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1195\n",
      "Epoch 19/100\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0836\n",
      "Epoch 20/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.0829\n",
      "Epoch 21/100\n",
      "8/8 [==============================] - 0s 804us/step - loss: 0.0819\n",
      "Epoch 22/100\n",
      "8/8 [==============================] - 0s 520us/step - loss: 0.0803\n",
      "Epoch 23/100\n",
      "8/8 [==============================] - 0s 464us/step - loss: 0.0779\n",
      "Epoch 24/100\n",
      "8/8 [==============================] - 0s 497us/step - loss: 0.0737\n",
      "Epoch 25/100\n",
      "8/8 [==============================] - 0s 684us/step - loss: 0.0693\n",
      "Epoch 26/100\n",
      "8/8 [==============================] - 0s 621us/step - loss: 0.3070\n",
      "Epoch 27/100\n",
      "8/8 [==============================] - 0s 647us/step - loss: 0.1119\n",
      "Epoch 28/100\n",
      "8/8 [==============================] - 0s 562us/step - loss: 0.0982\n",
      "Epoch 29/100\n",
      "8/8 [==============================] - 0s 510us/step - loss: 0.1594\n",
      "Epoch 30/100\n",
      "8/8 [==============================] - 0s 739us/step - loss: 0.0876\n",
      "Epoch 31/100\n",
      "8/8 [==============================] - 0s 695us/step - loss: 0.1265\n",
      "Epoch 32/100\n",
      "8/8 [==============================] - 0s 466us/step - loss: 0.1240\n",
      "Epoch 33/100\n",
      "8/8 [==============================] - 0s 910us/step - loss: 0.0817\n",
      "Epoch 34/100\n",
      "8/8 [==============================] - 0s 930us/step - loss: 0.0792\n",
      "Epoch 35/100\n",
      "8/8 [==============================] - 0s 797us/step - loss: 0.0751\n",
      "Epoch 36/100\n",
      "8/8 [==============================] - 0s 613us/step - loss: 0.0713\n",
      "Epoch 37/100\n",
      "8/8 [==============================] - 0s 495us/step - loss: 0.2465\n",
      "Epoch 38/100\n",
      "8/8 [==============================] - 0s 692us/step - loss: 0.1190\n",
      "Epoch 39/100\n",
      "8/8 [==============================] - 0s 349us/step - loss: 0.0818\n",
      "Epoch 40/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.0792\n",
      "Epoch 41/100\n",
      "8/8 [==============================] - 0s 966us/step - loss: 0.0750\n",
      "Epoch 42/100\n",
      "8/8 [==============================] - 0s 419us/step - loss: 0.0680\n",
      "Epoch 43/100\n",
      "8/8 [==============================] - 0s 678us/step - loss: 0.0604\n",
      "Epoch 44/100\n",
      "8/8 [==============================] - 0s 707us/step - loss: 0.2992\n",
      "Epoch 45/100\n",
      "8/8 [==============================] - 0s 576us/step - loss: 0.1086\n",
      "Epoch 46/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.0840\n",
      "Epoch 47/100\n",
      "8/8 [==============================] - 0s 476us/step - loss: 0.1493\n",
      "Epoch 48/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.0828\n",
      "Epoch 49/100\n",
      "8/8 [==============================] - 0s 757us/step - loss: 0.1083\n",
      "Epoch 50/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.0830\n",
      "Epoch 51/100\n",
      "8/8 [==============================] - 0s 573us/step - loss: 0.1506\n",
      "Epoch 52/100\n",
      "8/8 [==============================] - 0s 570us/step - loss: 0.0827\n",
      "Epoch 53/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.0808\n",
      "Epoch 54/100\n",
      "8/8 [==============================] - 0s 882us/step - loss: 0.1077\n",
      "Epoch 55/100\n",
      "8/8 [==============================] - 0s 858us/step - loss: 0.0870\n",
      "Epoch 56/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.1921\n",
      "Epoch 57/100\n",
      "8/8 [==============================] - 0s 478us/step - loss: 0.1008\n",
      "Epoch 58/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.1233\n",
      "Epoch 59/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.1237\n",
      "Epoch 60/100\n",
      "8/8 [==============================] - 0s 671us/step - loss: 0.0923\n",
      "Epoch 61/100\n",
      "8/8 [==============================] - 0s 823us/step - loss: 0.1477\n",
      "Epoch 62/100\n",
      "8/8 [==============================] - 0s 819us/step - loss: 0.0858\n",
      "Epoch 63/100\n",
      "8/8 [==============================] - 0s 510us/step - loss: 0.1118\n",
      "Restoring model weights from the end of the best epoch\n",
      "Epoch 00063: early stopping\n",
      "best epoch =  43\n",
      "smallest loss = 0.06043669581413269\n"
     ]
    }
   ],
   "source": [
    "historyData = model.fit(xarray,df.y3,epochs=100,callbacks=[es])\n",
    "\n",
    "loss_hist = historyData.history['loss']\n",
    "#The above line will return a dictionary, access it's info like this:\n",
    "best_epoch = np.argmin(historyData.history['loss']) + 1\n",
    "print ('best epoch = ', best_epoch)\n",
    "print('smallest loss =', np.min(loss_hist))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "8/8 [==============================] - 0s 752us/step - loss: 0.2992\n",
      "Epoch 2/100\n",
      "8/8 [==============================] - 0s 746us/step - loss: 0.0805\n",
      "Epoch 3/100\n",
      "8/8 [==============================] - 0s 519us/step - loss: 0.1172\n",
      "Epoch 4/100\n",
      "8/8 [==============================] - 0s 571us/step - loss: 0.1283\n",
      "Epoch 5/100\n",
      "8/8 [==============================] - 0s 847us/step - loss: 0.0836\n",
      "Epoch 6/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.0794\n",
      "Epoch 7/100\n",
      "8/8 [==============================] - 0s 818us/step - loss: 0.0741\n",
      "Epoch 8/100\n",
      "8/8 [==============================] - 0s 891us/step - loss: 0.0838\n",
      "Epoch 9/100\n",
      "8/8 [==============================] - 0s 996us/step - loss: 0.1414\n",
      "Epoch 10/100\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.1446\n",
      "Epoch 11/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0891\n",
      "Epoch 12/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.0834\n",
      "Epoch 13/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.0780\n",
      "Epoch 14/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.0704\n",
      "Epoch 15/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0573\n",
      "Epoch 16/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0438\n",
      "Epoch 17/100\n",
      "8/8 [==============================] - 0s 951us/step - loss: 0.3506\n",
      "Epoch 18/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0806\n",
      "Epoch 19/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.0772\n",
      "Epoch 20/100\n",
      "8/8 [==============================] - 0s 650us/step - loss: 0.0850\n",
      "Epoch 21/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.1409\n",
      "Epoch 22/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.1507\n",
      "Epoch 23/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.0738\n",
      "Epoch 24/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0634\n",
      "Epoch 25/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.0469\n",
      "Epoch 26/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.1660\n",
      "Epoch 27/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.2044\n",
      "Epoch 28/100\n",
      "8/8 [==============================] - 0s 655us/step - loss: 0.1209\n",
      "Epoch 29/100\n",
      "8/8 [==============================] - 0s 554us/step - loss: 0.1066\n",
      "Epoch 30/100\n",
      "8/8 [==============================] - 0s 557us/step - loss: 0.1639\n",
      "Epoch 31/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.0824\n",
      "Epoch 32/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.0808\n",
      "Epoch 33/100\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.0872\n",
      "Epoch 34/100\n",
      "8/8 [==============================] - 0s 713us/step - loss: 0.1364\n",
      "Epoch 35/100\n",
      "8/8 [==============================] - 0s 648us/step - loss: 0.1270\n",
      "Epoch 36/100\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0781\n",
      "Restoring model weights from the end of the best epoch\n",
      "Epoch 00036: early stopping\n",
      "best epoch =  16\n",
      "smallest loss = 0.04379470646381378\n"
     ]
    }
   ],
   "source": [
    "historyData = model.fit(xarray,df.y3,epochs=100,callbacks=[es])\n",
    "\n",
    "loss_hist = historyData.history['loss']\n",
    "#The above line will return a dictionary, access it's info like this:\n",
    "best_epoch = np.argmin(historyData.history['loss']) + 1\n",
    "print ('best epoch = ', best_epoch)\n",
    "print('smallest loss =', np.min(loss_hist))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "8/8 [==============================] - 1s 107ms/step - loss: 0.3506\n",
      "Epoch 2/200\n",
      "8/8 [==============================] - 0s 569us/step - loss: 0.2883\n",
      "Epoch 3/200\n",
      "8/8 [==============================] - 0s 481us/step - loss: 0.2450\n",
      "Epoch 4/200\n",
      "8/8 [==============================] - 0s 838us/step - loss: 0.2103\n",
      "Epoch 5/200\n",
      "8/8 [==============================] - 0s 816us/step - loss: 0.1805\n",
      "Epoch 6/200\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1540\n",
      "Epoch 7/200\n",
      "8/8 [==============================] - 0s 827us/step - loss: 0.1299\n",
      "Epoch 8/200\n",
      "8/8 [==============================] - 0s 920us/step - loss: 0.1076\n",
      "Epoch 9/200\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.0866\n",
      "Epoch 10/200\n",
      "8/8 [==============================] - 0s 846us/step - loss: 0.0717\n",
      "Epoch 11/200\n",
      "8/8 [==============================] - 0s 845us/step - loss: 0.0664\n",
      "Epoch 12/200\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0647\n",
      "Epoch 13/200\n",
      "8/8 [==============================] - 0s 672us/step - loss: 0.0635\n",
      "Epoch 14/200\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.0623\n",
      "Epoch 15/200\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0610\n",
      "Epoch 16/200\n",
      "8/8 [==============================] - 0s 831us/step - loss: 0.0595\n",
      "Epoch 17/200\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.0579\n",
      "Epoch 18/200\n",
      "8/8 [==============================] - 0s 627us/step - loss: 0.0562\n",
      "Epoch 19/200\n",
      "8/8 [==============================] - 0s 890us/step - loss: 0.0544\n",
      "Epoch 20/200\n",
      "8/8 [==============================] - 0s 589us/step - loss: 0.0525\n",
      "Epoch 21/200\n",
      "8/8 [==============================] - 0s 778us/step - loss: 0.0504\n",
      "Epoch 22/200\n",
      "8/8 [==============================] - 0s 702us/step - loss: 0.0483\n",
      "Epoch 23/200\n",
      "8/8 [==============================] - 0s 360us/step - loss: 0.0460\n",
      "Epoch 24/200\n",
      "8/8 [==============================] - 0s 324us/step - loss: 0.0436\n",
      "Epoch 25/200\n",
      "8/8 [==============================] - 0s 495us/step - loss: 0.0411\n",
      "Epoch 26/200\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.0385\n",
      "Epoch 27/200\n",
      "8/8 [==============================] - 0s 371us/step - loss: 0.0358\n",
      "Epoch 28/200\n",
      "8/8 [==============================] - 0s 379us/step - loss: 0.0330\n",
      "Epoch 29/200\n",
      "8/8 [==============================] - 0s 456us/step - loss: 0.0300\n",
      "Epoch 30/200\n",
      "8/8 [==============================] - 0s 718us/step - loss: 0.0295\n",
      "Epoch 31/200\n",
      "8/8 [==============================] - 0s 355us/step - loss: 0.0415\n",
      "Epoch 32/200\n",
      "8/8 [==============================] - 0s 430us/step - loss: 0.0245\n",
      "Epoch 33/200\n",
      "8/8 [==============================] - 0s 426us/step - loss: 0.0271\n",
      "Epoch 34/200\n",
      "8/8 [==============================] - 0s 504us/step - loss: 0.0342\n",
      "Epoch 35/200\n",
      "8/8 [==============================] - 0s 477us/step - loss: 0.0365\n",
      "Epoch 36/200\n",
      "8/8 [==============================] - 0s 538us/step - loss: 0.0217\n",
      "Epoch 37/200\n",
      "8/8 [==============================] - 0s 993us/step - loss: 0.0217\n",
      "Epoch 38/200\n",
      "8/8 [==============================] - 0s 397us/step - loss: 0.0178\n",
      "Epoch 39/200\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.0177\n",
      "Epoch 40/200\n",
      "8/8 [==============================] - 0s 645us/step - loss: 0.0282\n",
      "Epoch 41/200\n",
      "8/8 [==============================] - 0s 565us/step - loss: 0.0226\n",
      "Epoch 42/200\n",
      "8/8 [==============================] - 0s 442us/step - loss: 0.0292\n",
      "Epoch 43/200\n",
      "8/8 [==============================] - 0s 369us/step - loss: 0.0197\n",
      "Epoch 44/200\n",
      "8/8 [==============================] - 0s 392us/step - loss: 0.0303\n",
      "Epoch 45/200\n",
      "8/8 [==============================] - 0s 451us/step - loss: 0.0193\n",
      "Epoch 46/200\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.0198\n",
      "Epoch 47/200\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0314\n",
      "Epoch 48/200\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.0269\n",
      "Epoch 49/200\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.0206\n",
      "Epoch 50/200\n",
      "8/8 [==============================] - 0s 770us/step - loss: 0.0282\n",
      "Epoch 51/200\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0185\n",
      "Epoch 52/200\n",
      "8/8 [==============================] - 0s 578us/step - loss: 0.0294\n",
      "Epoch 53/200\n",
      "8/8 [==============================] - 0s 755us/step - loss: 0.0174\n",
      "Epoch 54/200\n",
      "8/8 [==============================] - 0s 538us/step - loss: 0.0189\n",
      "Epoch 55/200\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.0240\n",
      "Epoch 56/200\n",
      "8/8 [==============================] - 0s 441us/step - loss: 0.0239\n",
      "Epoch 57/200\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.0252\n",
      "Epoch 58/200\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.0218\n",
      "Epoch 59/200\n",
      "8/8 [==============================] - 0s 733us/step - loss: 0.0264\n",
      "Epoch 60/200\n",
      "8/8 [==============================] - 0s 576us/step - loss: 0.0199\n",
      "Epoch 61/200\n",
      "8/8 [==============================] - 0s 481us/step - loss: 0.0276\n",
      "Epoch 62/200\n",
      "8/8 [==============================] - 0s 682us/step - loss: 0.0197\n",
      "Epoch 63/200\n",
      "8/8 [==============================] - 0s 619us/step - loss: 0.0180\n",
      "Epoch 64/200\n",
      "8/8 [==============================] - 0s 654us/step - loss: 0.0312\n",
      "Epoch 65/200\n",
      "8/8 [==============================] - 0s 443us/step - loss: 0.0249\n",
      "Epoch 66/200\n",
      "8/8 [==============================] - 0s 986us/step - loss: 0.0214\n",
      "Epoch 67/200\n",
      "8/8 [==============================] - 0s 515us/step - loss: 0.0262\n",
      "Epoch 68/200\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0196\n",
      "Epoch 69/200\n",
      "8/8 [==============================] - 0s 813us/step - loss: 0.0274\n",
      "Epoch 70/200\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.0179\n",
      "Epoch 71/200\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0287\n",
      "Epoch 72/200\n",
      "8/8 [==============================] - 0s 375us/step - loss: 0.0177\n",
      "Epoch 73/200\n",
      "8/8 [==============================] - 0s 591us/step - loss: 0.0181\n",
      "Restoring model weights from the end of the best epoch\n",
      "Epoch 00073: early stopping\n",
      "best epoch =  53\n",
      "smallest loss = 0.017406456172466278\n"
     ]
    }
   ],
   "source": [
    "sgd = tf.keras.optimizers.RMSprop(learning_rate=0.01)\n",
    "model.compile(loss='mean_absolute_error',optimizer=sgd)\n",
    "\n",
    "historyData = model.fit(xarray,df.y3,epochs=200,callbacks=[es])\n",
    "\n",
    "loss_hist = historyData.history['loss']\n",
    "#The above line will return a dictionary, access it's info like this:\n",
    "best_epoch = np.argmin(historyData.history['loss']) + 1\n",
    "print ('best epoch = ', best_epoch)\n",
    "print('smallest loss =', np.min(loss_hist))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "8/8 [==============================] - 1s 111ms/step - loss: 0.0189\n",
      "\n",
      "Epoch 00001: loss improved from inf to 0.01893, saving model to best_model.SB\n",
      "Epoch 2/200\n",
      "8/8 [==============================] - 0s 311us/step - loss: 0.0178\n",
      "\n",
      "Epoch 00002: loss improved from 0.01893 to 0.01779, saving model to best_model.SB\n",
      "Epoch 3/200\n",
      "8/8 [==============================] - 0s 602us/step - loss: 0.0183\n",
      "\n",
      "Epoch 00003: loss did not improve from 0.01779\n",
      "Epoch 4/200\n",
      "8/8 [==============================] - 0s 353us/step - loss: 0.0169\n",
      "\n",
      "Epoch 00004: loss improved from 0.01779 to 0.01694, saving model to best_model.SB\n",
      "Epoch 5/200\n",
      "8/8 [==============================] - 0s 791us/step - loss: 0.0169\n",
      "\n",
      "Epoch 00005: loss improved from 0.01694 to 0.01692, saving model to best_model.SB\n",
      "Epoch 6/200\n",
      "8/8 [==============================] - 0s 358us/step - loss: 0.0180\n",
      "\n",
      "Epoch 00006: loss did not improve from 0.01692\n",
      "Epoch 7/200\n",
      "8/8 [==============================] - 0s 386us/step - loss: 0.0173\n",
      "\n",
      "Epoch 00007: loss did not improve from 0.01692\n",
      "Epoch 8/200\n",
      "8/8 [==============================] - 0s 726us/step - loss: 0.0171\n",
      "\n",
      "Epoch 00008: loss did not improve from 0.01692\n",
      "Epoch 9/200\n",
      "8/8 [==============================] - 0s 651us/step - loss: 0.0170\n",
      "\n",
      "Epoch 00009: loss did not improve from 0.01692\n",
      "Epoch 10/200\n",
      "8/8 [==============================] - 0s 635us/step - loss: 0.0168\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Applications/Software/Anaconda/anaconda3/lib/python3.6/site-packages/keras/engine/saving.py:165: UserWarning: TensorFlow optimizers do not make it possible to access optimizer attributes or optimizer state after instantiation. As a result, we cannot save the optimizer as part of the model save file.You will have to compile your model again after loading it. Prefer using a Keras optimizer instead (see keras.io/optimizers).\n",
      "  'TensorFlow optimizers do not '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00010: loss improved from 0.01692 to 0.01685, saving model to best_model.SB\n",
      "Epoch 11/200\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.0167\n",
      "\n",
      "Epoch 00011: loss improved from 0.01685 to 0.01673, saving model to best_model.SB\n",
      "Epoch 12/200\n",
      "8/8 [==============================] - 0s 708us/step - loss: 0.0169\n",
      "\n",
      "Epoch 00012: loss did not improve from 0.01673\n",
      "Epoch 13/200\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.0182\n",
      "\n",
      "Epoch 00013: loss did not improve from 0.01673\n",
      "Epoch 14/200\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.0173\n",
      "\n",
      "Epoch 00014: loss did not improve from 0.01673\n",
      "Epoch 15/200\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.0170\n",
      "\n",
      "Epoch 00015: loss did not improve from 0.01673\n",
      "Epoch 16/200\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.0170\n",
      "\n",
      "Epoch 00016: loss did not improve from 0.01673\n",
      "Epoch 17/200\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.0173\n",
      "\n",
      "Epoch 00017: loss did not improve from 0.01673\n",
      "Epoch 18/200\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.0173\n",
      "\n",
      "Epoch 00018: loss did not improve from 0.01673\n",
      "Epoch 19/200\n",
      "8/8 [==============================] - 0s 835us/step - loss: 0.0170\n",
      "\n",
      "Epoch 00019: loss did not improve from 0.01673\n",
      "Epoch 20/200\n",
      "8/8 [==============================] - 0s 769us/step - loss: 0.0169\n",
      "\n",
      "Epoch 00020: loss did not improve from 0.01673\n",
      "Epoch 21/200\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0168\n",
      "\n",
      "Epoch 00021: loss did not improve from 0.01673\n",
      "Epoch 22/200\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.0169\n",
      "\n",
      "Epoch 00022: loss did not improve from 0.01673\n",
      "Epoch 23/200\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.0177\n",
      "\n",
      "Epoch 00023: loss did not improve from 0.01673\n",
      "Epoch 24/200\n",
      "8/8 [==============================] - 0s 833us/step - loss: 0.0173\n",
      "\n",
      "Epoch 00024: loss did not improve from 0.01673\n",
      "Epoch 25/200\n",
      "8/8 [==============================] - 0s 882us/step - loss: 0.0169\n",
      "\n",
      "Epoch 00025: loss did not improve from 0.01673\n",
      "Epoch 26/200\n",
      "8/8 [==============================] - 0s 643us/step - loss: 0.0168\n",
      "\n",
      "Epoch 00026: loss did not improve from 0.01673\n",
      "Epoch 27/200\n",
      "8/8 [==============================] - 0s 874us/step - loss: 0.0167\n",
      "\n",
      "Epoch 00027: loss did not improve from 0.01673\n",
      "Epoch 28/200\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0167\n",
      "\n",
      "Epoch 00028: loss did not improve from 0.01673\n",
      "Epoch 29/200\n",
      "8/8 [==============================] - 0s 822us/step - loss: 0.0180\n",
      "\n",
      "Epoch 00029: loss did not improve from 0.01673\n",
      "Epoch 30/200\n",
      "8/8 [==============================] - 0s 810us/step - loss: 0.0171\n",
      "\n",
      "Epoch 00030: loss did not improve from 0.01673\n",
      "Epoch 31/200\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.0170\n",
      "Restoring model weights from the end of the best epoch\n",
      "\n",
      "Epoch 00031: loss did not improve from 0.01673\n",
      "Epoch 00031: early stopping\n",
      "best epoch =  11\n",
      "smallest loss = 0.01672607660293579\n"
     ]
    }
   ],
   "source": [
    "sgd = tf.keras.optimizers.RMSprop(learning_rate=0.001)\n",
    "model.compile(loss='mean_absolute_error',optimizer=sgd)\n",
    "\n",
    "historyData = model.fit(xarray,df.y3,epochs=200,callbacks=[es,mc])\n",
    "\n",
    "loss_hist = historyData.history['loss']\n",
    "#The above line will return a dictionary, access it's info like this:\n",
    "best_epoch = np.argmin(historyData.history['loss']) + 1\n",
    "print ('best epoch = ', best_epoch)\n",
    "print('smallest loss =', np.min(loss_hist))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.564119  ]\n",
      " [-0.45188004]\n",
      " [-0.5696211 ]]\n",
      "w01 =  1.564119 w02 =  -0.45188004 w03 =  -0.5696211\n",
      "[-0.52396977]\n",
      "b1 =  [-0.52396977]\n",
      "[[0.9981894]]\n",
      "w12 =  0.9981894\n",
      "[-0.11985482]\n",
      "b2 =  [-0.11985482]\n",
      "[[0.3756329]]\n",
      "w23 =  0.3756329\n",
      "[1.0126067]\n",
      "b3 =  [1.0126067]\n",
      "x01/20.2,  x02/14.5,   x03/308.0,  y3/32.4,  a3:\n",
      "0.9900990099009901 0.896551724137931 1.009090909090909 0.9558346964599856 [[0.9853929]]\n",
      "0.9900990099009901 1.0 1.0 0.9968828122588808 [[0.971232]]\n",
      "0.9900990099009901 1.0551724137931036 0.9935064935064936 0.9721922162896206 [[0.96427524]]\n",
      "1.0 0.896551724137931 1.009090909090909 0.9539829017622912 [[0.99082077]]\n",
      "0.9900990099009901 1.0 1.0 1.003055461251196 [[0.971232]]\n",
      "1.0 1.0551724137931036 0.9935064935064936 0.9691058917934631 [[0.96932137]]\n",
      "1.188118811881188 0.896551724137931 1.009090909090909 1.0984228881824636 [[1.1004896]]\n",
      "1.7821782178217822 1.0 1.0 1.4320545662170918 [[1.4333022]]\n",
      "  \n",
      "x01,  x02,   x03,  y3,  a3*32.4:\n",
      "20.0 13.0 310.8 30.969044165303533 [[31.926733]]\n",
      "20.0 14.5 308.0 32.29900311718774 [[31.467918]]\n",
      "20.0 15.3 306.0 31.499027807783705 [[31.24252]]\n",
      "20.2 13.0 310.8 30.909046017098234 [[32.102592]]\n",
      "20.0 14.5 308.0 32.498996944538746 [[31.467918]]\n",
      "20.2 15.3 306.0 31.3990308941082 [[31.406013]]\n",
      "23.999999999999996 13.0 310.8 35.58890157711182 [[35.655865]]\n",
      "36.0 14.5 308.0 46.398567945433776 [[46.43899]]\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "#For results of training network:\n",
    "\n",
    "#keras.layer.get_weights() function retrieves weight values\n",
    "first_layer_weights = model.layers[0].get_weights()[0]\n",
    "w01 = first_layer_weights[0][0]\n",
    "w02 = first_layer_weights[1][0]\n",
    "w03 = first_layer_weights[2][0]\n",
    "first_layer_bias  = model.layers[0].get_weights()[1]\n",
    "b1 = first_layer_bias\n",
    "second_layer_weights = model.layers[1].get_weights()[0]\n",
    "w12 = second_layer_weights[0][0]\n",
    "second_layer_bias  = model.layers[1].get_weights()[1]\n",
    "b2 = second_layer_bias\n",
    "third_layer_weights = model.layers[2].get_weights()[0]\n",
    "w23 = third_layer_weights[0][0]\n",
    "third_layer_bias  = model.layers[2].get_weights()[1]\n",
    "b3 = third_layer_bias\n",
    "\n",
    "#print weights and biases\n",
    "print (first_layer_weights)\n",
    "print ('w01 = ', w01, 'w02 = ', w02, 'w03 = ', w03)\n",
    "print (first_layer_bias)\n",
    "print ('b1 = ', b1)\n",
    "print (second_layer_weights)\n",
    "print ('w12 = ', w12)\n",
    "print (second_layer_bias)\n",
    "print ('b2 = ', b2)\n",
    "print (third_layer_weights)\n",
    "print ('w23 = ', w23)\n",
    "print (third_layer_bias)\n",
    "print ('b3 = ', b3)\n",
    "\n",
    "#use model.predict() function to print model predictions for data conditions\n",
    "xarray= np.array(xdata)\n",
    "print ('x01/20.2,  x02/14.5,   x03/308.0,  y3/32.4,  a3:')\n",
    "test = []\n",
    "for i in range(0,8): \n",
    "    test = [[xarray[i][0], xarray[i][1], xarray[i][2]]]\n",
    "    testarray = np.array(test)\n",
    "    a3 = model.predict(testarray)\n",
    "    print (xarray[i][0], xarray[i][1], xarray[i][2], df.y3[i], a3)\n",
    "print('  ')\n",
    "print ('x01,  x02,   x03,  y3,  a3*32.4:')\n",
    "for i in range(0,8): \n",
    "    test = [[xarray[i][0], xarray[i][1], xarray[i][2]]]\n",
    "    testarray = np.array(test)\n",
    "    a3 = model.predict(testarray)\n",
    "    print (xarray[i][0]*20.2, xarray[i][1]*14.5, xarray[i][2]*308.0, df.y3[i]*32.4, a3*32.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictedvals = np.array([32.4*model.predict(np.array([xarray[i]]))[0][0] for i in range(8)])\n",
    "actualvals = np.array(32.4*df.y3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "fig,ax = plt.subplots()\n",
    "ax.scatter(actualvals,predictedvals)\n",
    "ax.set_xscale('log')\n",
    "ax.set_yscale('log')\n",
    "ax.set_xlabel(\"Actual values\")\n",
    "ax.set_ylabel(\"Predicted values\")\n",
    "ax.set_title(\"Comparison of predicted values with actual values\")\n",
    "ax.grid()\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([31.92673087, 31.4679167 , 31.24251781, 32.1025928 , 31.4679167 ,\n",
       "       31.40601239, 35.65586357, 46.43899012])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictedvals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([30.96904417, 32.29900312, 31.49902781, 30.90904602, 32.49899694,\n",
       "       31.39903089, 35.58890158, 46.39856795])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "actualvals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
